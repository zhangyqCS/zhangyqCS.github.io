<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>社会网络分析-基础知识</title>
      <link href="/2020/03/21/%E7%A4%BE%E4%BC%9A%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
      <url>/2020/03/21/%E7%A4%BE%E4%BC%9A%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;<strong>社会网络分析</strong>是研究一组行为者的关系的研究方法，一组行为者可以是人、社区、群体等，他们的关系模式反映出的现象或数据是网络分析的焦点。社会网络分析以数据挖掘为基础，采用可视化的图表以及社会网络结构的形式表示，运用这种研究方法，可以建立社会关系模型、发现社群内部行动者之间的各种社会关系。</p><hr><h3 id="1-网络中关键节点的识别"><a href="#1-网络中关键节点的识别" class="headerlink" title="1. 网络中关键节点的识别"></a>1. 网络中关键节点的识别</h3><h4 id="1-1-衡量方式"><a href="#1-1-衡量方式" class="headerlink" title="1.1 衡量方式"></a>1.1 衡量方式</h4><ul><li><p>度：最简单也是最直观的衡量方式，节点的度数越大，说明与该节点相连的节点就越多，该节点越重要，例如微博大V。<strong>优点</strong>是计算简单，<strong>缺点</strong>是缺乏全局考虑，仅考虑了1度关联的结点数，没有考虑关联结点的重要性，例如某大V购买了很多僵尸粉，但这个“大V”对正常用户可能影响很小。</p></li><li><p>介数：节点的介数被定义为<strong>网络中所有的最短路径经过该节点的数目</strong>，介数越高，说明网络中任意两个节点的关系与该节点关系越大，即该节点影响力越大。<strong>优点</strong>是相比度，介数考虑节点在整个网络中的重要程度，<strong>缺点</strong>是计算时间复杂度高，在节点较多的复杂网络中不适用。</p></li><li><p>核度：节点的核度被定义为，对网络从外层一层层地剥离直到没有节点，该节点处于被剥离的位置。节点的核度越大，越是最后被剥离，说明它越处于网络的中心位置，也就越重要。值得注意的是，<strong>并不是度越大的节点，其核度越大</strong>。<strong>优点</strong>是相比度的局限性，核度考虑了节点在全局的重要程度，并且没有计算介数复杂度这么高。缺点是划分粒度太粗，导致很多感觉上并不属于同一层级的节点被同时剥离，即每一次剥离的节点很多。</p></li></ul><h4 id="1-2-经典算法-PageRank"><a href="#1-2-经典算法-PageRank" class="headerlink" title="1.2 经典算法-PageRank"></a>1.2 经典算法-PageRank</h4><p>&emsp;&emsp;简单来说，一个网页的影响力=所有入链集合的页面的加权影响力之和：$PR(u) = \sum_{v \in B_u} \frac{PR(v)}{L(v)}$。其中$u$为待评估的页面，$B_u$为页面$u$的入链集合。针对入链集合中的任意页面$v$，它能带给$u$的影响力是其自身的影响力除以$v$页面的出链数量，即页面$v$把自身的影响力$PR(v)$平均分配给它的出链。出链会给被链接的页面赋予影响力，当我们统计一个网页出链的数量，也就能统计该页面的跳转概率。</p><div align=center><img src="http://tva4.sinaimg.cn/large/006Ahf5Fly1gd2mmco03fj309n06fmz1.jpg"></div><p>&emsp;&emsp;这个例子中，A有三个出链分别连接到B、C和D，当用户访问A页面时，就有机会跳转到B、C、D页面的可能，其概率均为$\frac{1}{3}$。同理，我们可求得A、B、C、D四个页面的转移矩阵：$$M = \begin{bmatrix}<br>{0} &amp; { 1/2 } &amp; {1} &amp; {0}\<br>{ 1/3 } &amp; {0} &amp; {0} &amp; { 1/2 }\<br>{ 1/3 } &amp; {0} &amp; {0} &amp; { 1/2 }\<br>{ 1/3 } &amp; { 1/2 } &amp; {0} &amp; {0}\<br>\end{bmatrix}$$</p><p>我们假设初始时，页面A、B、C、D影响力是相同的，即：$$ w_0 = \begin{bmatrix}<br>{ 1/4 }\<br>{ 1/4 }\<br>{ 1/4 }\<br>{ 1/4 }\<br>\end{bmatrix}$$<br>当第一次转移后，各页面的影响力$w_1$变为：$$ w_1 = \begin{bmatrix}<br>{0} &amp; { 1/2 } &amp; {1} &amp; {0}\<br>{ 1/3 } &amp; {0} &amp; {0} &amp; { 1/2 }\<br>{ 1/3 } &amp; {0} &amp; {0} &amp; { 1/2 }\<br>{ 1/3 } &amp; { 1/2 } &amp; {0} &amp; {0}\<br>\end{bmatrix} \begin{bmatrix}<br>{ 1/4 }\<br>{ 1/4 }\<br>{ 1/4 }\<br>{ 1/4 }\<br>\end{bmatrix} = \begin{bmatrix}<br>{ 9/24 }\<br>{ 5/24 }\<br>{ 5/24 }\<br>{ 5/24 }\<br>\end{bmatrix}$$</p><p>然后，我们再用转移矩阵乘以$w_1$得到$w_2$，一直迭代，直至$w_n$影响力不再发生变化，所求的$w_n$对于各个页面最终的影响力。至此我们模拟了一个简化的PageRank的计算过程，实际情况中可能面临如下两个问题：</p><ul><li>等级泄露(Rank Leak)：如果一个页面没有出链，就像一个黑洞，吸收了其他页面的影响力而不释放，最终会导致其他页面影响力为0。</li><li>等级沉没(Rank Sink)：如果一个页面只有出链，没有入链，计算的过程迭代下来，会导致这个页面的$PR$值为0。</li></ul><p>&emsp;&emsp;为了解决简化模型中等级泄露和等级沉没的问题，原作者提出了PageRank的随机浏览模型，该模型假设：<strong>用户并不都是按照跳转链接的方式来上网，还有一种可能就是不论当前处于哪个页面，都有一定概率访问其他页面，比如用户直接输入网址访问某个页面，虽然这种概率比链接访问的概率小</strong>。所以该模型定义了阻尼因子$d$代表了用户按链接跳转访问页面的，通常可以取固定值$d=0.85$，则影响力计算公式可改为：$PR(u) = \frac{1-d}{N} + d \sum_{v \in B_u} \frac{PR(v)}{L(v)}$。其中$N$为总页面数。在一定程度上可以缓解等级沉没和等级泄露的问题。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://www.jianshu.com/p/37964b2f538f" target="_blank" rel="noopener">网络中关键节点的识别-简书</a></li><li><a href="https://www.cnblogs.com/jpcflyer/p/11180263.html" target="_blank" rel="noopener">机器学习经典算法之PageRank-个人博客</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 社会网络分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 社会网络分析 </tag>
            
            <tag> 理论知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《统计自然语言处理》笔记-CH07-自动分词、命名实体识别与词性标注</title>
      <link href="/2020/02/15/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH07-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D%E3%80%81%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E4%B8%8E%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/"/>
      <url>/2020/02/15/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH07-%E8%87%AA%E5%8A%A8%E5%88%86%E8%AF%8D%E3%80%81%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E4%B8%8E%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="1-汉语自动分词中的基本问题"><a href="#1-汉语自动分词中的基本问题" class="headerlink" title="1. 汉语自动分词中的基本问题"></a>1. 汉语自动分词中的基本问题</h2><ul><li>汉语分词规范问题：”词是什么”(词的抽象定义)及”什么是词”(词的具体界定)两个基本问题至今没有一个权威公认的定义。</li><li>歧义切分问题：汉语词词语边界的歧义切分问题比较复杂，处理此问题往往需要进行复杂的上下文语义分析。</li><li>未登录词问题：又称集外词(out of vocabulary, OOV)。</li></ul><h2 id="2-汉语分词方法"><a href="#2-汉语分词方法" class="headerlink" title="2. 汉语分词方法"></a>2. 汉语分词方法</h2><h3 id="2-1-N-最短路径方法"><a href="#2-1-N-最短路径方法" class="headerlink" title="2.1 N-最短路径方法"></a>2.1 N-最短路径方法</h3><p>&emsp;&emsp;由于汉语分词的歧义切分和未登录词识别两个问题，分词过程分成两个阶段：首先<strong>采用切分算法对句子词语进行初步切分</strong>，得到一个相对最好的粗分结果，然后<strong>再进行歧义消除和未登录词识别</strong>。基于N-最短路径方法的汉语词语<strong>粗分</strong>模型基本思想是：根据词典，找出字符串中所有可能的词，构造词语切分有向无环图，每个词对应图中的一条有向边，并赋予权值。然后针对该切分图，在起点到终点的所有路径中，求出<strong>长度按严格升序排列</strong>的路径集合作为相应的粗分结果集。</p><p>&emsp;&emsp;假设待分字串$S = c_1 c_2 … c_n$，其中$c_i(i=1, 2, …, n)$为单个汉字，建立一个结点数为$n+1$的切分有向无环图$G$，各结点依次编号$V_0, V_1, …, V_n$。按以下步骤建立$G$所有可能的边：</p><ol><li>相邻结点$V_{k-1}, V_k(1 \leq k \leq n)$之间建立有向边，边的权值为$L_k$，边对应的词默认为$c_k$。</li><li>如果$w = c_i c_{i+1} … c_j$是词表中的词，则结点$V_{i-1}, V_j$之间建立有向边，边的长度为$L_w$，边对应的词为$w$。</li></ol><p>&emsp;&emsp;考虑到切分有向无环图$G$中每条边权重的影响，该方法可分为非统计粗分模型和统计粗分模型。非统计粗分模型假定$G$中<strong>所有词的权重都是相等的</strong>，权重均为1。假设NSP为结点$V_0$到$V_n$的前$N$个最短路的集合，RS是最终的N-最短路径粗分结果集，则问题转化为求解有向无环图的集合NSP。求解NSP集合可以采取贪心技术，一种算法是基于求解单源最短路径问题的Dijkstra贪心算法的简单扩展。改进之处在于：每个结点处记录N个最短路径值，并记录相应路径上当前结点的前驱。</p><h3 id="2-2-基于词的n元语法模型的分词方法"><a href="#2-2-基于词的n元语法模型的分词方法" class="headerlink" title="2.2 基于词的n元语法模型的分词方法"></a>2.2 基于词的n元语法模型的分词方法</h3><p>&emsp;&emsp;基于词的n元语法模型是典型的<strong>生成式模型</strong>。其基本思想是：首先根据词典对句子进行简单匹配，找出所有可能的词典词，然后将它们和所有单个字作为节点，构造n元的切分词图，边表示路径，边上的n元概率表示代价，最后利用相关搜索算法(如维特比算法)从图中找出代价最小的路径作为最后的分词结果。</p><p>为了给自动分词任务一个明确的定义，汉语词可定义为以下4类：</p><ul><li>待切分文本中能与分词词表中任意一个词相匹配的字段为一个词。</li><li>文本中任意一个经词法派生出来的词或短语为一个词。</li><li>文本中被明确定义的任意一个实体名词是一个词。</li></ul><p>假设随机变量$S$为一个汉子序列，$W$是$S$上所有可能切分出来的词序列，分词过程就是求解使条件概率$p(W \mid S)$最大的切分出来的词序列$W^* $，即$$W^* = \argmax_W \frac{p(W) p(S \mid W)}{p(S)} = \argmax_W p(W) p(S \mid W) \tag{7-1}$$ $p(W)$即为语言模型。将$p(S \mid W)$称为生成模型。</p><h3 id="2-3-由字构词的汉语分词方法"><a href="#2-3-由字构词的汉语分词方法" class="headerlink" title="2.3 由字构词的汉语分词方法"></a>2.3 由字构词的汉语分词方法</h3><p>&emsp;&emsp;由字构词(character-based tagging)的汉语分词方法基本思想是，<strong>将分词过程看作字的分类问题</strong>。以往的分词方法中，无论是基于规则的方法还是基于统计的方法，一般都依赖于一个事先编制的词表，自动分词过程就是通过查表做出切分的决策。与此相反，由字构词的分词方法认为每个字在构造一个特定的词语时都占据着一个确定的构词位置(即词位)，如词首(B)、词中(M)、词尾(E)和单独成词(S)。该方法的优势是，能够平衡地看待词表词和未登录词的识别问题，文本中的词表词和未登录词都是用统一的字标注过程来实现的。</p><h2 id="3-命名实体识别"><a href="#3-命名实体识别" class="headerlink" title="3. 命名实体识别"></a>3. 命名实体识别</h2><h3 id="3-1-基于CRF的命名实体识别方法"><a href="#3-1-基于CRF的命名实体识别方法" class="headerlink" title="3.1 基于CRF的命名实体识别方法"></a>3.1 基于CRF的命名实体识别方法</h3><p>&emsp;&emsp;该方法也是把NER过程看作是一个序列标注问题。其基本思路是：将给定的文本首先进行分词处理，然后对人名、简单地名、和简单的机构名进行识别，最后识别复合地名和复合机构名。在训练阶段首先需要将分词语料的标记符号转化成用于NER的标记，然后再确定特征模板。特征模板一般采用当前位置的前后$n (n \geq 1)$个位置上的字(或词、字母等，统称为”字串”)及其标记表示，即设置观察窗口。由于不同的命名实体一般出现在不同的上下文语境中，因此对于不同的NER一般采用不同的特征模板。</p><h3 id="3-2-基于多特征的命名实体识别方法"><a href="#3-2-基于多特征的命名实体识别方法" class="headerlink" title="3.2 基于多特征的命名实体识别方法"></a>3.2 基于多特征的命名实体识别方法</h3><p>&emsp;&emsp;在NER中，不同方法都试图充分发现和利用<strong>实体所在的上下文特征</strong>和<strong>实体的内部特征</strong>，只不过特征的粒度大小不同。基于多特征的NER方法是在分词和词性标注的基础上进一步进行NER，由<strong>词形上下文特征、词性上下文特征、词形实体模型和词性实体模型</strong>4个子模型组成。其中，词形上下文模型估计在给定词形上下文语境中产生实体的概率；词性上下文模型估计在给定词性上下文语境中产生实体的概率；词形实体模型估计在给定实体类型的情况下词形串作为实体的概率；词性实体模型估计在给定实体类型的情况下词性串作为实体的概率。</p><ol><li><strong>模型描述</strong></li></ol><p>&emsp;&emsp;词形包括以下几种情况：字典中任何一个字或词单独构成一类；人名(Per)、人名简称(Aper)、地名(Loc)、地名简称(Aloc)、机构名(Org)、时间词(Tim)和数量词(Num)各定义一类，共$|V|+ 7$个词形。词性采用PKU计算语言学研究所开发的<a href="http://icl.pku.edu.cn/nlp-tools/catetkset.html" target="_blank" rel="noopener">汉语文本词性标注集</a>，加上人名简称词性和地名简称词性，共47个。</p><p>&emsp;&emsp;NER可以看作一个序列化数据的标注问题，输入是带有词性标记的词序列$$WT = w_1/t_1 \ \ \ w_2/t_2 \ \ \ … \ \ \ w_n/t_n \tag{7-2}$$ 其中$n$是句子被切分出来的词个数，$t_i$是标注的词$w_i$的词性。在分词和词性标注的基础上进行NER的过程就是对部分词语进行拆分、组合(确定实体边界)和重新分类(确定实体类别)的过程，最后输出一个最优的”词性/词性”序列$WC^* / TC^* $：$$WC^* / TC^* = wc_1 / tc_1 \ \ \ wc_2/tc_2 \ \ \ … \ \ \ wc_m / tc_m \tag{7-3}$$ 由式(7-2)计算最优”词形/词性”序列的方法有三种：</p><ul><li>词形模型：根据词形序列$W$产生候选命名实体，用维特比算法确定最优词形序列$WC^* $。</li><li>词性模型：根据词性序列$T$产生候选命名实体，用维特比算法确定最优词性序列$TC^*$。</li><li>混合模型：基于多特征的识别算法即属于此：$$(WC^* , TC^* ) = \argmax_{(WC, TC)} p(WC, TC \mid W, T) \approx  \argmax_{(WC, TC)} p(WC) \times p(W \mid WC) \times [p(TC) \times p(T \mid TC)]^\beta \tag{7-4}$$ </li></ul><p>其中$\beta$是平衡因子，模型(7-4)由四部分组成，分别称为：词形上下文模型$p(WC)$、词性上下文模型$p(TC)$、实体词形模型$p(W \mid WC)$和实体词性模型$p(T \mid TC)$。实体词形模型和实体词性模型统称为实体模型。</p><ol start="2"><li><strong>词形和词性上下文模型</strong></li></ol><p>词形和词性上下文模型可采用三元语法模型近似描述：$$p(WC) \approx \prod_{i=1}^m p(wc_i \mid wc_{i-2} \ wc_{i-1}) \tag{7-5}$$ $$p(TC) \approx \prod_{i=1}^m p(tc_i \mid tc_{i-2} \ tc_{i-1}) \tag{7-6}$$ 当$i=1$时取一元语法概率，当$i=2$时取二元语法概率。</p><ol start="3"><li><strong>实体模型</strong></li></ol><p>考虑到每一类命名实体都具有不同的内部特征，因此不能用一个统一的模型刻画人名、地名和机构名等实体模型。</p><h2 id="4-词性标注"><a href="#4-词性标注" class="headerlink" title="4. 词性标注"></a>4. 词性标注</h2><h3 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h3><p>词性(part-of-speech)是词汇基本的语法属性，通常也称为词类。汉语词性标注的主要三个难点：</p><ul><li>汉语是一种缺乏词形态变化的语言，词的类别不能像印欧语那样直接从词的形态变化上判别。</li><li>常用词兼类现象严重，造成汉语文本中词类歧义排除的任务量大，面广且复杂多样。</li><li>至今没有公认的统一的汉语词类划分标准。</li></ul><h3 id="4-2-基于统计模型的词性标注方法"><a href="#4-2-基于统计模型的词性标注方法" class="headerlink" title="4.2 基于统计模型的词性标注方法"></a>4.2 基于统计模型的词性标注方法</h3><p>&emsp;&emsp;Jelinek方法用最大似然估计来估算概率$p(w^k \mid t^i)$以初始化HMM(<strong>随机初始化HMM参数会使词性标注问题过于缺乏限制</strong>)，并假设每个词与每个可能的词性标记出现的概率相等。</p><p>&emsp;&emsp;还有一种方法是采用将词汇划分为若干等价类，以类为单位进行参数估计，从而避免了为每个单词单独调整参数，大大减少了参数的总个数。</p><h3 id="4-3-基于规则的词性标注方法"><a href="#4-3-基于规则的词性标注方法" class="headerlink" title="4.3 基于规则的词性标注方法"></a>4.3 基于规则的词性标注方法</h3><p>&emsp;&emsp;基于规则的词性标注方法基本思想是按兼类词搭配关系和上下文语境建造词类消歧规则。基于转换规则的错误驱动的学习方法可以<strong>通过机器学习的方法自动获取规则</strong>。基本思想是首先运用初始状态标注器标识未标注的文本，由此产生已标注文本，再与正确的标注文本(人工)相比较。由于初始标注器标注的文本一般会含有错误，学习器通过将这些标注文本与正确的标注文本比较，可以学习一些转换规则，从而形成一个排序的转换规则集，使其能够修正已标注的文本，使标注结果更接近参考答案。</p><h3 id="待更新…"><a href="#待更新…" class="headerlink" title="待更新…"></a>待更新…</h3>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP基础 </tag>
            
            <tag> 《统计自然语言处理》 </tag>
            
            <tag> 读书笔记 </tag>
            
            <tag> 分词 </tag>
            
            <tag> 命名实体识别 </tag>
            
            <tag> 词性标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《统计自然语言处理》笔记-CH06-概率图模型</title>
      <link href="/2020/02/13/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH06-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/02/13/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH06-%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>&emsp;&emsp;概率图模型(probabilistic graphical models)在概率模型的基础上，使用了基于图的方法来表示概率分布(或者概率密度、密度函数)，是一种通用化的不确定性知识表示和处理方法。在概率图模型的表达中，结点表示变量，结点之间直接相连的边表示相应变量之间的概率关系。</p><div align=center><img src="http://tva2.sinaimg.cn/large/006Ahf5Fly1gbuoqmv0hqj30ng0d4n1i.jpg" width="600" height="320"></div><p>&emsp;&emsp;动态贝叶斯网络(dynamic Bayesian networks, DBN)用于处理<strong>随时间变化</strong>的动态系统中的推断和预测问题。其中HMM在语音识别、汉语自动分词与词性标注和统计机器翻译等NLP任务中得到了广泛应用；卡尔曼滤波器则在信号处理领域有广泛用途。CRF广泛应用于序列标注、特征选择、机器翻译等任务。</p><div align=center><img src="http://tvax4.sinaimg.cn/large/006Ahf5Fly1gbup7didvxj30jn093n00.jpg" height="300" width="700"></div><p>&emsp;&emsp;图6-2诠释了NLP中概率图模型的演变过程。<strong>横向</strong>：由点到线(序列结构)、到面(图结构)。以朴素贝叶斯模型为基础的HMM用于处理线性序列问题，有向图模型用于解决一般图问题；以逻辑回归模型(即NLP中ME模型)为基础的线性链式条件随机场用于解决”线式”序列问题，通用条件随机场用于解决一般图问题。<strong>纵向</strong>：在一定条件下生成式模型(generative model)转变为判别式模型(discriminative model)。</p><p>&emsp;&emsp;生成式模型以<strong>输出序列$y$按照一定的规律生成输入序列$x$</strong> 为假设，针对联合分布$p(x, y)$进行建模，并且通过估计使生成概率最大的生成序列来获取$y$。生成式模型是所有变量的全概率模型，因此可以模拟(“生成”)所有变量的值。这类模型的<strong>优点</strong>是：处理单类问题时比较灵活，模型变量之间的关系比较清除，模型可以通过增量学习获得，可用于数据不完整的情况。其<strong>缺点</strong>在于模型的推导和学习比较复杂。</p><p>&emsp;&emsp;判别式模型直接对后验概率$p(y \mid x)$进行建模。其<strong>优点</strong>是：处理多类问题或分辨某一类与其他类之间的差异时比较灵活，模型简单，容易建立和学习。其<strong>缺点</strong>在于模型的描述能力有限，变量之间的关系不清楚，而且大多数判别式模型是有监督学习方法，不能扩展成无监督的学习方法。</p><blockquote><p>&emsp;&emsp;直观地理解，假设我们已有一批训练数据$(x,y)$，$x$是属性集合，$y$是类别标签。这时有一个新的样本$x$，我们想要预测它的类别$y$。<strong>判别式和生成式的最终的目的都是求得最大的条件概率$p(y \mid x)$作为新样本的类别标签，只是在训练阶段的目标不同</strong>。判别式目标是直接对$p(y \mid x)$进行建模，划定一个分界面。生成式目标是对变量的联合分布$p(x, y)$进行建模，再根据贝叶斯公式，选择使结果最优的作为最终分布$p(x, y)$。</p></blockquote><h2 id="2-贝叶斯网络"><a href="#2-贝叶斯网络" class="headerlink" title="2. 贝叶斯网络"></a>2. 贝叶斯网络</h2><p>&emsp;&emsp;贝叶斯网络又称信度网络或信念网络(belief networks)，是一种基于概率推理的数学模型，其理论基础是贝叶斯公式。其目的是通过概率推理处理不确定性和不完整性问题。</p><p>&emsp;&emsp;形式上，一个贝叶斯网络就是一个<strong>有向无环图</strong>，结点之间的有向边表示条件依存关系，子结点依存于父结点。两个结点没有连接关系表示两个随机变量能够在某些特定情况下条件独立，而<strong>有连接关系表示两个随机变量在任何条件下都不存在条件独立</strong>。构造贝叶斯网络涉及表示、推断和学习三个方面的问题。</p><ol><li>表示：在某一随机变量的集合$x = {X_1, …, X_n}$上给出其联合概率分布$P$。主要问题是所有概率情况太多($=|变量取值|^n$)。</li><li>推断：贝叶斯网络可以回答关于变量的查询。在已知某些证据的情况下计算变量的后验分布的过程称作概率推理。常用方法包括变量消除法(variable elimination)和团树法(clique tree)。变量消除法的基本任务是计算条件概率$p(X_Q \mid X_E = x)$。其基本思想是通过分步计算不同变量的边缘分布按顺序逐个消除未观察到的非查询变量。团树法使用更全局化的数据结构调度各种操作，以获得更加有益的计算代价。</li><li>学习：包括参数学习和结构学习。参数学习的目的是<strong>变量依存强度估计</strong>。</li></ol><h2 id="3-马尔可夫模型"><a href="#3-马尔可夫模型" class="headerlink" title="3. 马尔可夫模型"></a>3. 马尔可夫模型</h2><blockquote><ul><li>随机变量：本质上是一个函数，完成随机试验的结果到实数的一个映射。</li><li>随机过程：现实中常出现某个事物满足一定的随机分布，但其随机分布随时间的变化而变化。这个总体过程称为随机过程。</li></ul></blockquote><p>&emsp;&emsp;马尔可夫过程(Markov process)是一类随机过程，它的原始模型是马尔可夫链。该过程的特性：<strong>在已知目前状态(现在)的条件下，它未来的演变(将来)不依赖于它以往的演变(过去)</strong>。每个状态的转移只依赖于之前的n个状态，这个过程被称为一个n阶的模型。最简单的马尔可夫过程就是一阶过程，<strong>每一个状态的转移只依赖于其之前的那一个状态</strong>，也叫作马尔可夫性质。满足该性质的随机过程为马尔可夫模型。</p><h2 id="4-隐马尔可夫模型"><a href="#4-隐马尔可夫模型" class="headerlink" title="4. 隐马尔可夫模型"></a>4. 隐马尔可夫模型</h2><p>&emsp;&emsp;举个例子，假定一暗室有$N$个口袋，每个口袋有$M$种不同颜色的球。一个实验员根据某一概率分布随机选取一个初始口袋，从中随机取出一个球，并向室外的人报告球的颜色。然后再选择另一个口袋，再取出一个球，重复该过程。对于室外的人，<strong>可观察的过程只是不同颜色的球的序列，而口袋的序列是不可观察的</strong>。这个过程中，每个口袋对应HMM中的状态，球的颜色对应HMM中状态的输出符号。从一个口袋转向另一个口袋对应于状态转换，从口袋取出球的颜色对应于从一个状态输出的观察符号。可以看出HMM由一下几部分组成：</p><ul><li>模型中状态的数目$N$；</li><li>从每个状态可能输出的不同符号的数目$M$；</li><li>状态转移矩阵$A = {a_{ij}}$；</li><li>从状态$s_j$观察到输出符号$v_k$的概率分布矩阵$B = {b_j(k)}$；</li><li>初始状态概率分布$\pi = {\pi_i}$。</li></ul><p>&emsp;&emsp;一般地，一个HMM记为一个五元组$\mu =(S, K, A, B, \pi)$。其中$S$为状态的集合，$K$为输出符号的集合。HMM中有三个基本问题：</p><ol><li>估计问题：给定一个观察序列$O = O_1 O_2 … 0_T$和模型$\mu = (A, B, \pi)$，如何快速地计算出给定模型$\mu$情况下，观测序列$O$的概率，即$p(O \mid \mu)$？</li><li>序列问题：给定一个观察序列$O = O_1 O_2 … 0_T$和模型$\mu = (A, B, \pi)$，如何快速有效地选择在一定意义下”最优”的状态序列$Q = q_1 q_2 … q_T$，使得该状态序列”最好地解释”观察序列？</li><li>训练问题或参数估计问题：给定一个观察序列$O = O_1 O_2 … 0_T$，如何根据最大似然估计来求模型的参数值？即如何调节模型$\mu = (A, B, \pi)$的参数，使得$p(O \mid \mu)$最大？</li></ol><p>以下描述的前后向算法及参数估计将给出这三个问题的解决方案。</p><h3 id="4-1-求解观察序列的概率"><a href="#4-1-求解观察序列的概率" class="headerlink" title="4.1 求解观察序列的概率"></a>4.1 求解观察序列的概率</h3><p>&emsp;&emsp;问题一其实就是解码问题，可以通过枚举所有可能的状态序列来求解，但计算量会出现”指数爆炸”。为此人们提出了前向算法或前向计算过程(forward procedure)，利用动态规划的方法求解问题。</p><p>HMM中DP问题一般用格架(trellis或lattice)的组织形式描述。为了实现前向算法，需要定义一个前向变量$a_t(i)$。</p><ul><li>前向变量$a_t(i)$是在时间$t$，HMM输出了序列$O_1 O_2  … O_t$，并且位于状态$s_i$的概率：$$a_t(i) = p(O_1 O_2  … O_t, q_t = s_i \mid \mu) \tag{6-1}$$ </li></ul><p>前向算法的主要思想是，如果可以快速地计算$a_t(i)$，那么就可以根据$a_t(i)$计算出$p(O \mid \mu)$。因为$$p(O \mid \mu) = \sum_{s_i} p(O_1 O_2 … O_T, q_T = s_i \mid \mu) = \sum_{i=1}^N a_T(i) \tag{6-2}$$ 在前向算法中，求$a_t(i)$的实现思想是：在时间$t+1$的前向变量可以根据在时间$t$的前向变量$a_t(1), a_t(2), …, a_t(N)$的值来归纳计算：$$a_{t+1}(j) = (\sum_{i=1}^N a_t(i) a_{ij}) b_j(O_{T+1}) \tag{6-3}$$ 算法伪代码如下：</p><div align=left><img src="http://tva3.sinaimg.cn/large/006Ahf5Fly1gbw5k81z9wj30gp06s3zc.jpg" height="180" width="420"></div><p>问题一还可以通过后向算法来解决。后向变量$\beta_t(i)$定义如下：</p><ul><li>后向变量$\beta_t(i)$是在给定了HMM模型$\mu = (A, B, \pi)$，并且在时间$t$状态为$s_i$的条件下，输出观察序列$O_{t+1} O_{t+2} … O_T$的概率：$$\beta_t(i) = p(O_{t+1} O_{t+2} … O_T \mid q_t = s_i, \mu) \tag{6-4}$$</li></ul><p>利用DP，归纳关系如下：$$\beta_t(i) = \sum_{j=1}^N a_{ij}b_j(O_{t+1}) \beta_{t+1}(j) \tag{6-5}$$ 算法伪代码如下：</p><div align=left><img src="http://tva2.sinaimg.cn/large/006Ahf5Fly1gbw60jksz8j30hj06saay.jpg" height="180" width="420"></div><h3 id="4-2-维特比算法"><a href="#4-2-维特比算法" class="headerlink" title="4.2 维特比算法"></a>4.2 维特比算法</h3><p>&emsp;&emsp;维特比(Viterbi)算法用于求解HMM的第二个问题。”最优状态序列”定义为，在给定模型$\mu$和观察序列$O$的条件下，使条件概率$p(Q \mid O, \mu)$最大的状态序列，即：$$\hat Q = \argmax_Q p(Q \mid O, \mu)$$ 维特比算法运用DP的搜索算法求解最优状态序列，定义维特比变量$\delta_t(i)$:</p><ul><li>维特比变量$\delta_t(i)$是在时间$t$时，HMM沿着某一条路径到达状态$s_i$，并输出观察序列$O_1 O_2 … O_t$的最大概率：$$\delta_t(i) = \max_{q_1, q_2, …, q_{t-1}} p(q_1, q_2, …, q_t = s_i, O_1 O_2 … O_t \mid \mu) \tag{6-6}$$</li></ul><p>其递推关系为：$$\delta_{t+1}(i) = \max_j[ \delta_t(j) \cdot a_{ji}] \cdot b_i(O_{t+1})\tag{6-7}$$ 为了记录在时间$t$时，HMM通过哪一条概率最大的路径到达状态$s_i$，维特比算法设置了另外一个变量$\psi_t(i)$用于路径记忆，<strong>让$\psi_t(i)$记录该路径上状态$s_i$的前一个(在时间$t-1$)状态</strong>。算法伪代码如下：</p><div align=left><img src="http://tvax1.sinaimg.cn/large/006Ahf5Fly1gbw7d4kui0j30i10cb40f.jpg" height="300" width="480"></div><h3 id="4-3-HMM的参数估计"><a href="#4-3-HMM的参数估计" class="headerlink" title="4.3 HMM的参数估计"></a>4.3 HMM的参数估计</h3><p>&emsp;&emsp;由于HMM中的状态序列$Q$是观察不到的(隐变量)，传统最大似然估计方法不可行。<strong>期望最大化</strong>(expectation maximization, EM)算法可以用于含有隐变量的统计模型的参数最大似然估计。其基本思想是，初始时随机地给模型的参数赋值(该赋值遵循模型对参数的限制)，得到模型$\mu_0$，然后根据$\mu_0$可以得到模型中隐变量的期望值。例如，从$\mu_0$得到从某一状态转移到另一状态的期望次数，用期望次数来替代实际次数，这样可以得到模型参数的新估值，得到模型$\mu_1$，然后迭代执行这个过程，直到参数收敛于最大似然估计值。</p><p>&emsp;&emsp;这种迭代爬山算法可以局部地使得$p(O \mid \mu)$最大化。鲍姆-韦尔奇(Baum-Welch)算法或称前向后向算法用于具体实现这种EM算法。给定HMM的参数$\mu$和观察序列$O=O_1 O_2 … O_T$，在时间$t$位于状态$s_i$，时间$t+1$位于状态$s_j$的概率$\xi_t(i, j)$可由下式计算获得：$$\xi_t(i, j) = \frac{p(q_t = s_i, q_{t+1} = s_j, O \mid \mu)}{p(O \mid \mu)} = \frac{a_t(i)a_{ij}b_j(O_{t+1}) \beta_{t+1}(j)}{\sum_{i=1}^N \sum_{j=1}^N a_t(i)a_{ij}b_j(O_{t+1}) \beta_{t+1}(j)} \tag{6-8}$$ 给定HMM的$\mu$和观察序列$O=O_1 O_2 … O_T$，在时间$t$位于状态$s_i$的概率$\gamma_t(i)$为：$$\gamma_t(i) = \sum_{j=1}^N \xi_t(i, j) \tag{6-9}$$ 算法伪代码如下：</p><div align=left><img src="http://tva2.sinaimg.cn/large/006Ahf5Fly1gbw9p075lwj30lh0clwh7.jpg" height="350" width="560"></div><h2 id="5-层次化的隐马尔可夫模型"><a href="#5-层次化的隐马尔可夫模型" class="headerlink" title="5. 层次化的隐马尔可夫模型"></a>5. 层次化的隐马尔可夫模型</h2><p>&emsp;&emsp;在NLP等应用中，由于处理序列具有递归性，尤其当序列长度较大时，HMM的复杂度将急剧增大。所以有人提出了层次化隐马尔可夫模型(hierarchical hidden Markov models, HHMM)。HMMM是由多层随机过程构成的，每个状态本身就是一个独立的HHMM，因此<strong>一个HHMM的状态产生一个观察序列，而不是一个观察符</strong>。HHMM通过状态转移递归地产生观察序列，一个状态可以激活下层状态中的某一状态，而被激活的状态又可以激活再下层的状态，直至到达某个特定状态递归过程才结束。该特定状态称为生产状态(production state)，<strong>只有生产状态才能通过常规的HMM机制，及根据输出符号的概率分布产生可观察的输出符号</strong>。不直接产生可观察符的隐藏状态称为内部状态。不同层次之间的状态转移叫垂直转移，同一层上状态转移叫水平转移。像HMM一样，HHMM也有三个基本问题需要解决。</p><h2 id="6-马尔可夫网络"><a href="#6-马尔可夫网络" class="headerlink" title="6. 马尔可夫网络"></a>6. 马尔可夫网络</h2><p>&emsp;&emsp;马尔可夫网络也可用于表示变量之间的依赖关系，但是它与贝叶斯网络有所不同。一方面，它可以表示贝叶斯网络无法表示的一些依赖关系，如循环依赖；另一方面，它不能表示贝叶斯网络能够表示的某些关系，如推导关系。马尔可夫网络是一组有<strong>马尔可夫性质</strong>的随机变量的联合概率分布模型，它由一个无向图G和定义于G上的势函数组成。无向图G的每条边表示直接相连的两个随机变量之间的一种依赖关系。</p><ul><li>如果一个子图中任意两个结点之间都有边相连，这个子图就是一个完全子图。又称为团(clique)。</li></ul><p>&emsp;&emsp;在无向图中，不用条件概率密度对模型进行参数化，而是使用一种称为团势能(clique potentials)的参数化因子。团势能是定义在团上的非负实函数。<strong>每个团都对应一个势函数，表示团的一个状态</strong>。一般用$X_c$表示团$C$中所有的结点，用$\phi(X_c)$表示团势能。一般团势能定义为：$\phi(X_c) = exp{- E(X_c)}$，其中$E(X_c)$称为$X_c$的能量函数。</p><p>&emsp;&emsp;如果分布$p_\phi(x_1, x_2, …, x_n)$的图模型可以表示为一个马尔可夫网络$H$，当$C$是$H$上完全子图的集合时，我们说$H$上的分布$p_\phi(x_1, x_2, …, x_n)$可以用$C$的团势能函数进行因子化：$\phi = \phi_1(X_{c_1}), …, \phi_K(X_{c_k})}$。$p_\phi(x_1, x_2, …, x_n)$可以看作$H$上的一个吉布斯分布(Gibbs distribution)，其概率分布密度为：$$p(x_1, x_2, …, x_n) = \frac{1}{Z} \prod_{i=1}^K \phi_i(X_{c_i})$$ 其中，$Z$是一个归一化常量，称为划分函数：$$Z = \sum_{x_1, x_2, …, x_n} \prod_{i=1}^K \phi_i(X_{c_i})$$</p><h2 id="7-最大熵模型"><a href="#7-最大熵模型" class="headerlink" title="7. 最大熵模型"></a>7. 最大熵模型</h2><ul><li>原理：在只掌握关于未知分布的部分信息的情况下，符合已知知识的概率分布可能有多个，但使其熵值最大的概率分布最真实地反映了事件的分布情况。</li></ul><h2 id="8-条件随机场"><a href="#8-条件随机场" class="headerlink" title="8. 条件随机场"></a>8. 条件随机场</h2><p>&emsp;&emsp;CRF用来标注和划分序列结构数据的概率化结构模型，即对于给定的输出标识序列$Y$和观测序列$X$，CRF通过定义条件概率$p(Y \mid X)$来描述模型(<strong>判别式</strong>)。</p><ul><li>定义：设$G= (V, E)$为一个无向图，$V$为结点集合，$E$为无向边的集合。$Y = {Y_v \mid v \in V}$，即$V$中的每一个结点对应与一个随机变量$Y_v$，其取值范围为可能的标记集合${y}$。如果以观察序列$X$为条件，每一个随机变量$Y_v$都满足以下马尔可夫特性：$$p(Y_v \mid X, Y_w, w \neq v) = p(Y_v \mid X, Y_w, w \sim v) \tag{6-10}$$ 其中$w \sim v$表示两个结点在图$G$中是临近结点。</li></ul><p>不过一般说CRF为序列建模，就专指CRF线性链(linear chain CRF)：</p><div align=center><img src="http://tvax1.sinaimg.cn/large/006Ahf5Fly1gbx57zc2c2j30jz04x0tl.jpg" height="150" width="600"></div><p>&emsp;&emsp;显然，观察序列$X$的元素之间并不存在图结构，因为这里只是将观察序列$X$作为条件，并不对其作任何独立性假设。在给定观察序列$X$时，某个特定标记序列$Y$的概率可以定义为：$$exp(\sum_j \lambda_j t_j(y_{i-1}, y_i, X, i) + \sum_k \mu_k s_k(y_i, X, i)) \tag{6-11}$$ 其中，$t_j(y_{i-1}, y_i, X, i)$是转移函数，表示对于观察序列其标注序列在$i$及$i-1$位置上标记的转移概率；$s_K(y_i, X, i)$是状态函数，表示对于观察序列其$i$位置的标记概率；$\lambda_j, \mu_k$分别是权重，需要从训练样本中估计出来。其中，</p><p>$$t_j(y_{i-1}, y_i, X, i) = \begin{cases} b(X, i), y_{i-1}和y_i满足某种搭配条件\ 0, 否则 \end{cases} b(X, i) = \begin{cases} 1, X在i的位置为某个特定词\ 0, 否则 \end{cases}$$ 为了便于描述，可以将状态函数写成如下形式：$s(y_i, X, i) = s(y_{i-1}, y_i, X, i)$。这样特征函数可以统一表示为：$$F_j(Y, X) = \sum_{i=1}^n f_j(y_{i-1}, y_i, X, i) \tag{6-12}$$ 其中，每个局部特征函数$f$表示状态特征$s$或转移函数$t$。由此，CRF定义的条件概率可以由下式给出：$$p(Y \mid X, \lambda) = \frac{1}{Z(X)} exp(\lambda_j \cdot F_j(Y, X)) = \frac{exp(\lambda_j \cdot F_j(Y, X)}{\sum_Y exp(\lambda_j \cdot F_j(Y, X)} \tag{6-13}$$ CRF也需要解决三个基本问题：<strong>特征的选取、参数训练和解</strong>码。</p><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><ul><li><a href="https://www.zhihu.com/question/35866596/answer/236886066" target="_blank" rel="noopener">如何用简单易懂的例子解释CRF？它和HMM有什么区别？</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP基础 </tag>
            
            <tag> 《统计自然语言处理》 </tag>
            
            <tag> 读书笔记 </tag>
            
            <tag> 隐马尔可夫模型 </tag>
            
            <tag> 条件随机场 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《统计自然语言处理》笔记-CH05-语言模型</title>
      <link href="/2020/02/11/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH05-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
      <url>/2020/02/11/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH05-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;语言模型(language model, LM)在NLP中占有重要地位，尤其在基于统计模型的语音识别、机器翻译、汉语自动分词和句法分析等相关研究中得到了广泛应用。目前主要采用的是<strong>n元语法模型</strong>(n-gram model)，这种模型<strong>简单、直接，但同时也因为数据缺乏而必须采取平滑(smoothing)算法</strong>。</p><h2 id="1-n元语法"><a href="#1-n元语法" class="headerlink" title="1. n元语法"></a>1. n元语法</h2><p>&emsp;&emsp;一个语言模型通常构建为字符串$s$的概率分布$p(s)$，这里$p(s)$试图反映的是字符串$s$作为一个句子出现的概率。<strong>与语言学中不同，语言模型与句子是否合乎语法是没有关系的</strong>，即使一个句子完全合乎语法逻辑，我们仍然可以认为它出现的概率接近零。</p><p>&emsp;&emsp;对于一个由$l$个基元(“基元”可以为字、词或短语等，为了表述方便，只用”词”来通指)构成的句子$s = w_1 w_2 … w_l$，其概率计算公式可以表示为：$$p(s) = p(w_1) p(w_2 \mid w_1) … p(w_l \mid w_1 w_2 … w_{l-1}) = \prod_{i=1}^l p(w_i \mid w_1 w_2 … w_{i-1}) \tag{5-1}$$ 一般地，前$i-1$个词$w_1 w_2 … w_{i-1}$称为第$i$个词的”历史(history)”。在这种计算方法中，随着历史长度的增加，不同的历史数目按指数级增长。实际上绝大多数历史根本就不可能在训练数据中出现。因此，为了解决这个问题，可以<strong>将历史$w_1 w_2 … w_{i-1}$按照某个法则映射到等价类$E(w_1 w_2 … w_{i-1})$</strong>，而这个等价类的数目远远小于不同历史的数目。如果假定：$$p(w_i \mid w_1 w_2 … w_{i-1}) = p(w_i \mid E(w_1 w_2 … w_{i-1})) \tag{5-2}$$ 那么自由参数的数目将大大减少。</p><p>&emsp;&emsp;有很多方法可以将历史划分成等价类，其中，一种比较实际的做法是，将两个历史$w_{i-n+2} … w_{i-1} w_i$和$v_{k-n+2} … v_{k-1} v_k$映射到同一个等价类，当且仅当这两个历史最近的$n-1 (1 \leq n \leq l)$个词相同。满足上述条件的语言模型称为<strong>n元语法</strong>。通常情况下n的取值不能太大，否则自由参数过多的问题依然存在。在实际应用中常取n=3。</p><ul><li>n=2时，二元语法模型被称为一阶马尔可夫链(Markov chain)；n=3时，三元语法模型被称为二阶马尔可夫链。</li></ul><p>&emsp;&emsp;以二元语法为例，可以近似认为，一个词的概率只依赖于它前面的一个词，那么，$$p(s) = \prod_{i=1}^l p(w_i \mid w_1 w_2 … w_{i-1}) \approx \prod_{i=1}^l p(w_i \mid w_{i-1}) \tag{5-3}$$ 为了使得$p(w_1 \mid w_0)$有意义，我们在句子开头加一个句首标志&lt;BOS&gt;。另外为使所有的字符串的概率之和为1，需要在句尾加一个标记&lt;EOS&gt;。</p><p>&emsp;&emsp;为了估计$p(w_i \mid w_{i-1})$条件概率，可以简单地计算二元语法$w_{i-1} w_i$在某一文本中出现的频率，然后归一化，即最大似然估计方法。$$p(w_i \mid w_{i-1}) = \frac{c(w_{i-1} w_i)}{\sum_{w_i} c(w_{i-1} w_i)} \tag{5-4}$$</p><h2 id="2-语言模型性能评价"><a href="#2-语言模型性能评价" class="headerlink" title="2. 语言模型性能评价"></a>2. 语言模型性能评价</h2><p>&emsp;&emsp;评价一个语言模型最常用的度量就是根据模型计算出的<strong>测试数据的概率</strong>，或利用<strong>交叉熵</strong>和<strong>困惑度</strong>等派生测度。对于一个平滑过的概率为$p(w_i \mid w_{i-n+1}^{i-1})$的n元语法模型，对于句子$(t_1, t_2, …, t_T)$构成的测试集$T$，可以计算$T$中所有句子概率的乘积来计算测试集的概率：$$p(T) = \prod_{i=1}^{t_T} p(t_i)$$</p><h2 id="3-数据平滑"><a href="#3-数据平滑" class="headerlink" title="3. 数据平滑"></a>3. 数据平滑</h2><h3 id="3-1-问题的提出"><a href="#3-1-问题的提出" class="headerlink" title="3.1 问题的提出"></a>3.1 问题的提出</h3><p>&emsp;&emsp;如果依据给定的训练语料$S$，计算出某个句子$s$的概率为0，显然这样不够准确，因为句子$s$总有出现的可能，其概率应该大于0。因而必须分配给所有可能出现的字符串一个非零的概率值来避免错误发生。平滑(smoothing)技术就是用来解决这类问题。也常称为数据平滑。其基本思想是<strong>提高低概率(如零概率)，降低高概率，尽量使概率分布趋于均匀</strong>。最简单的平滑技术就是假设每个句子出现的次数比实际出现的次数多一次。</p><h3 id="3-2-加法平滑方法"><a href="#3-2-加法平滑方法" class="headerlink" title="3.2 加法平滑方法"></a>3.2 加法平滑方法</h3><p>&emsp;&emsp;实际应用中最简单的平滑技术之一就是加法平滑方法(additive smoothing)。基本思想是假设每个句子比实际出现情况多发生$\delta (0 \leq \delta \leq 1)$次。$$p_{add}(w_i \mid w_{i-n+1}^{i-1}) = \frac{\delta + c(w_{i-n+1}^i)}{\delta |V| + \sum_{w_i} c(w_{i-n+1}^i)} \tag{5-5}$$</p><h3 id="3-3-古德-图灵-Good-Turing-估计法"><a href="#3-3-古德-图灵-Good-Turing-估计法" class="headerlink" title="3.3 古德-图灵(Good-Turing)估计法"></a>3.3 古德-图灵(Good-Turing)估计法</h3><p>&emsp;&emsp;该方法是很多平滑技术的核心。其基本思路是：<strong>对于任何一个出现$r$次的n元语法，都假设它出现了$r^* $次，这里$r^* = (r+1) \frac{n_{r+1}}{n_r}$</strong>。其中$n_r$是训练语料中恰好出现$r$次的n元语法的数目。要把这个统计数转化为概率，只需要进行归一化：$p_r = \frac{r^* }{N}$。其中$$N = \sum_{r=0}^{\infty} n_r r^* = \sum_{r=0}^{\infty} (r+1)n_{r+1} = \sum_{r=1}^{\infty} n_r r \tag{5-6}$$ 也就是说，$N$等于这个分布最初的计数。这样样本中所有事件的概率之和为：$\sum_{r &gt; 0} n_r p_r = 1 - \frac{n_1}{N} &lt; 1$。因此有$\frac{n_1}{N}$的概率剩余量可以分配给所有未见事件($r=0$的事件)。</p><ul><li>该方法不能直接用于估计$n_r=0$的n-gram概率。</li><li>不能实现高阶模型与低阶模型的结合，而高低阶模型的结合通常是获得较好的平滑效果所必须的。</li></ul><h3 id="3-4-Katz平滑方法"><a href="#3-4-Katz平滑方法" class="headerlink" title="3.4 Katz平滑方法"></a>3.4 Katz平滑方法</h3><p>&emsp;&emsp;Katz平滑方法通过加入高阶模型与低阶模型的结合，扩展了古德-图灵估计方法。以二元语法模型为例，对于一个出现次数为$r = c(w_{i-1}^i)$的二元语法$w_{i-1}^i$，使用下式估算修正的计数：$$c_{katz}(w_{i-1}^i) = \begin{cases} d_r r&amp; \text{r&gt;0}\ \alpha(w_{i-1}) p_{ML}(w_i)&amp; \text{r=0}\end{cases} \tag{5-7}$$ 折扣率$d_r$近似地等于$\frac{r^* }{r}$，这个值是由古德-图灵方法预测的。<strong>从非零计数中减去的计数量，根据低一阶的分布，被分配给了计数为零的高阶语法</strong>。$\alpha(w_{i-1})$的值定义为：$$\alpha(w_{i-1}) = \frac{1- \sum_{w_i: c(w_{i-1}^i)&gt;0} p_{katz}(w_i \mid w_{i-1})}{1- \sum_{w_i: c(w_{i-1}^i) &gt; 0} p_{ML}(w_i)}$$</p><p>&emsp;&emsp;Katz平滑方法属于后备(back-off)平滑方法。这种方法的中心思想是：当某一事件在样本中出现的频率大于k时，运用最大似然估计经过减值来估计其概率。当某一事件的频率小于k时，使用低阶的语法模型作为代替高阶语法模型的后备，而这种代替必须受归一化因子$\alpha$的作用。</p><h3 id="3-5-Jelinek-Mercer平滑方法"><a href="#3-5-Jelinek-Mercer平滑方法" class="headerlink" title="3.5 Jelinek-Mercer平滑方法"></a>3.5 Jelinek-Mercer平滑方法</h3><p>&emsp;&emsp;假定要在一批训练预料上构建的二元语法模型，其中有两对词的共现数为0，c(send the) = c(send thou) = 0。按照加法平滑方法和古德-图灵平滑方法可以得到：$$p(the \mid send) = p(thou \mid send)$$ 但直观上应该有：$$p(the \mid send) &gt; p(thou \mid send)$$ 因为the出现的频率高得多。为了利用这种情况，一种处理方法是将二元语法模型和一元语法模型进行线性插值：$$p_{interp}(w_i \mid w_{i-1}) = \lambda p_{ML}(w_i \mid w_{i-1}) + (1- \lambda)p_{ML}(w_i), 0 \leq \lambda \leq 1$$</p><ul><li>一般使用低阶的n元模型向高阶n元模型插值是有效的。</li></ul><h2 id="4-语言模型自适应方法"><a href="#4-语言模型自适应方法" class="headerlink" title="4. 语言模型自适应方法"></a>4. 语言模型自适应方法</h2><p>&emsp;&emsp;语言模型<strong>对跨领域的脆弱性</strong>(brittleness across domains )和<strong>独立性假设的无效性</strong>(false independence assumption)是两个最明显的问题。为了提高语言模型对语料的领域、主题、类型等因素的适应性，提出了自适应语言模型(adaptive language model)的概念。</p><h3 id="4-1-基于缓存的语言模型"><a href="#4-1-基于缓存的语言模型" class="headerlink" title="4.1 基于缓存的语言模型"></a>4.1 基于缓存的语言模型</h3><p>&emsp;&emsp;基于缓存的语言模型自适应方法针对的问题是，<strong>在文本中刚刚出现过的一些词在后边的句子中再次出现的可能性往往比较大，比标准的n元语法模型预测的概率要大</strong>。其基本思路是，语言模型通过n元语法的线性插值求得：$$\hat p(w_i \mid w_l^{i-1}) = \lambda \hat p_{Cache}(w_i \mid w_l^{i-1}) + (1 - \lambda) \hat p_{n-gram}(w_i \mid w_{i-n+1}^{i-1}) \tag{5-8}$$ 常用的方法是在缓存中保留前面的K个单词，每个词$w_i$的概率(缓存概率)用其在缓存中出现的相对频率计算得出：$$\hat p_{Cache} (w_i \mid w_l^{i-1})= \frac{1}{K} \sum_{j = i - K}^{i-1} I_{ { w_j = w_i} } \tag{5-9}$$ 其中$I_{\epsilon}$为指示器函数，如果$\epsilon$表示的情况出现则$I_{\epsilon} = 1$，否则为0。</p><p>&emsp;&emsp;然而这样的做法有明显的缺陷。<strong>缓存中一个词的重要性独立于该词与当前词的距离</strong>。研究表明，缓存中每个词对当前词的影响随着与该词距离的增大呈指数级衰减，式(5-9)可改写为：$$\hat p_{Cache} (w_i \mid w_l^{i-1}) = \beta \sum_{j=l}^{i-1} I_{ {w_i = w_j} } e^{-\alpha(i-j)}\tag{5-10}$$ 其中，$\alpha$为衰减率；$\beta$为归一化常数，以使得概率和为1。</p><h3 id="4-2-基于混合方法的语言模型"><a href="#4-2-基于混合方法的语言模型" class="headerlink" title="4.2 基于混合方法的语言模型"></a>4.2 基于混合方法的语言模型</h3><p>&emsp;&emsp;基于混合方法的语言模型针对的问题是，<strong>由于大规模训练语料本身是异源的(heterogenous)，即来自不同领域，而测试语料一般是同源的(homogenous)</strong>。其基本思想是，将语言模型划分成n个子模型$M_1, M_2, …, M_n$，整个语言模型的概率通过线性插值得到：$$\hat p(w_i \mid w_l^{i-1}) = \sum_{j=1}^n \lambda_j \hat p_{M_j}(w_i \mid w_l^{i-1}) \tag{5-11}$$ 该适应方法针对测试语料的实现过程包括以下步骤：</p><ol><li>对训练语料按来源、主题或类型等聚类；</li><li>在模型运行时识别测试语料的主题或主题的集合；</li><li>确定适当的训练语料子集，并利用这些语料建立特定的语言模型；</li><li>利用针对各个语料子集的特定语言模型和线性插值公式(5-11)获得整个语言模型。</li></ol><h3 id="4-3-基于最大熵的语言模型"><a href="#4-3-基于最大熵的语言模型" class="headerlink" title="4.3 基于最大熵的语言模型"></a>4.3 基于最大熵的语言模型</h3><p>&emsp;&emsp;其基本思路是，<strong>通过结合不同信息源的信息构建一个语言模型，每个信息源提供一组关于模型参数的约束条件，在所有满足约束的模型中，选择熵最大的模型</strong>。</p><p>&emsp;&emsp;举个例子，考虑两个语言模型$M_1, M_2$，$M_1$是标准的二元模型，$M_2$是距离为2的二元模型(distance-2 bigram)。$$\hat p_{M_1}(w_i \mid w_l^{i-1}) = f(w_i, w_{i-1}) \tag{5-12}$$ $$\hat p_{M_2}(w_i \mid w_l^{i-1}) = g(w_i, w_{i-2}) \tag{5-13}$$ 可以用线性插值方法取两个概率的平均值，用后备方法选择其中一个进行数据平滑。最大熵原则将所有的信息组合成一个模型，对于该模型的约束<strong>并不是</strong>让式(5-12)和(5-13)对于所有可能的历史都成立，而是放宽了限制，使它们<strong>在训练数据上平均成立</strong>即可。</p>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP基础 </tag>
            
            <tag> 《统计自然语言处理》 </tag>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《统计自然语言处理》笔记-CH04-语料库与语言知识库</title>
      <link href="/2020/02/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH04-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/"/>
      <url>/2020/02/10/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH04-%E8%AF%AD%E6%96%99%E5%BA%93%E4%B8%8E%E8%AF%AD%E8%A8%80%E7%9F%A5%E8%AF%86%E5%BA%93/</url>
      
        <content type="html"><![CDATA[<h2 id="1-语料库技术"><a href="#1-语料库技术" class="headerlink" title="1. 语料库技术"></a>1. 语料库技术</h2><h3 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h3><p>&emsp;&emsp;语料库(corpus base)就是存放语言材料的数据库。<strong>语料库语言学</strong>(corpus linguistics)是研究自然语言机读文本(或称”电子文本”)的采集、存储、标注、检索、统计等方法的一门学问，其<strong>目的</strong>是通过对客观存在的大规模真实文本中的语言事实进行定量分析，为语言学研究或NLP系统开发提供支持。语料库语言学<strong>研究的内容</strong>大致包括：</p><ol><li>语料库的建设与编撰；</li><li>语料库的加工与管理；</li><li>语料库的应用，包括在语言学研究(言语、词汇和语义研究等)中的应用和在NLP中的应用。</li></ol><h3 id="1-2-语料库的类型"><a href="#1-2-语料库的类型" class="headerlink" title="1.2 语料库的类型"></a>1.2 语料库的类型</h3><p>&emsp;&emsp;根据不同的划分标准，语料库可以分为多种类型。这里主要介绍以语料代表性和平衡性为主要区分依据的”平衡语料库与平行语料库”、以语料库用途为主要区分依据的”通用语料库与专用语料库”、以预料分布时间为主要区分依据”共时语料库与历时语料库”和以语料库内容加工程度划分的”生语料与标注语料库”。</p><ul><li><strong>平衡语料库与平行语料库</strong>：平衡语料库着重考虑的是语料的代表性与平衡性。平行语料一般有两种含义：一种是指在同一种语言的语料上的平行，其平行性表现为语料选取的时间、对象、比例、文本数、文本长度等几乎是一致的；另一种理解是指两种或多种语言之间的平行采样和加工。如机器翻译中的双语对齐语料库。</li><li><strong>通用语料库与专用语料库</strong>：所谓的通用语料库实际上与平衡语料库是从不同角度看问题的结果。为了某种专门的目的，只采集某一特定领域、特定地区、特定时间、特定类型的语料构成的语料库就是专用语料库。</li><li><strong>共时语料库与历时语料库</strong>：所谓共时语料库是为了对语言进行共时研究而建立的语料库。共时研究是指研究一个共时平面中的元素间关系，如研究大树的横断面所见的细胞之间的关系；所谓历时语料库是为了对语言进行历时研究而建立的语料库。历时研究是研究一个历时切面中元素间关系的演化，如研究大树的纵剖面所见的细胞关系的演变。</li><li><strong>生语料库与标注语料库</strong>：生语料是指没有经过任何加工处理的原始语料数据。标注语料库是指经过加工处理、标注了特定信息的语料库。</li></ul><h3 id="1-3-汉语语料库建设中的问题"><a href="#1-3-汉语语料库建设中的问题" class="headerlink" title="1.3 汉语语料库建设中的问题"></a>1.3 汉语语料库建设中的问题</h3><ol><li>语料库建设的规范问题</li><li>产权保护和国家语料库建设问题</li></ol><h2 id="2-语言知识库"><a href="#2-语言知识库" class="headerlink" title="2. 语言知识库"></a>2. 语言知识库</h2><p>&emsp;&emsp;”语言知识库”比”语料库”包含更广泛的内容。语言知识库可分为两种不同的类型：一类是词典、规则库、语义概念库等，其中的语言知识表示是显性的，可采用形式化结构描述；另一类语言知识存在于语料库之中，每个语言单位的出现，其范畴、意义、用法都是确定的。语料库的本体是文本，每个语句都是线性的非结构化的文字序列，其包含的知识是隐性的。</p><h3 id="2-1-WordNet"><a href="#2-1-WordNet" class="headerlink" title="2.1 WordNet"></a>2.1 WordNet</h3><p>&emsp;&emsp;WordNet是一个英语机读词汇知识库，其建立的<strong>三个基本前提</strong>：(1)”可分离性假设(separability hypothesis)”，即语言的词汇成分可以被离析出来并专门针对它加以研究。(2)”模式假设(patterning hypothesis)”：一个人不可能掌握他运用一种语言所需的的所有词汇，除非他能利用词义中存在的系统的模式和词义之间的关系。(3)”广泛性假设(comprehensiveness hypothesis)”：计算语言学如果希望能像人一样处理自然语言，就需要像人一样存储尽可能多的词汇知识，<a href="http://wordnet.princeton.edu/" target="_blank" rel="noopener">主页</a>。</p><h3 id="2-2-FrameNet"><a href="#2-2-FrameNet" class="headerlink" title="2.2 FrameNet"></a>2.2 FrameNet</h3><p>&emsp;&emsp;FrameNet是基于框架语义学(frame semantics)并以语料库为基础建立的在线英语词汇资源库。其目的是通过样本句子的计算机辅助标注和标注结果的自动表格化显示，来验证每个词在每种语义下语义和句法结合的可能性范围。</p><h3 id="2-3-概念层次网络"><a href="#2-3-概念层次网络" class="headerlink" title="2.3 概念层次网络"></a>2.3 概念层次网络</h3><p>&emsp;&emsp;概念层次网络(Hierarchical Network of Concepts, HNC)是面向整个自然语言理解的理论框架。HNC理论是一个关于语言概念空间的理论，但它只研究这个空间的部分特征，即与自然语言理解过程有关的特性。</p><p>&emsp;&emsp;<strong>局部联想脉络</strong>是HNC理论的基本内容之一，它由<strong>五元组、语义网络和概念组合结构</strong>组成，其基本思路和做法是：把概念分为抽象概念和具体概念，对抽象概念用语义网络和五元组来表达，对具体概念采取挂靠展开近似表达的方法。</p><p>&emsp;&emsp;在HNC理论中，五元组、语义网络和概念组合结构用来表达抽象概念。五元组是指{动态、静态、属性、值、效应}五大特性，它们是词性的基元，用以表达概念的外在表现。任何概念都具有五元组特性。语义网络用以表达概念的内涵。语义网络是树状的分层结构，每一层有若干结点，每个结点代表一个<strong>概念基元</strong>(而不是词)，每层的若干结点分别用连续的数字标记，网络中的任一结点都可以通过从最高层开始到结点结束的一串数字唯一确定和表示，这种数字串称为层次符号。结点代表的概念基元通过不同方式的组合就可以表达各种各样的、无数的概念，而不受语种限制。概念组合结构用以表达概念基元的组合方式。五元组符号、层次符号和概念组合结构符号组合起来就构成了HNC的概念表示式。</p><p>&emsp;&emsp;HNC设计了抽象概念的三大语义网络：基本概念语义网络、基元概念语义网络和逻辑概念语义网络。三大语义网络是HNC理论的核心，是”概念基元”的聚类和系统，而绝非”词”的分类。其设计思想有两个主要来源：一是奎廉(Quillian)的语义网络、菲尔墨的格语法和山克的概念从属理论；二是汉语的”字义基元化，词义组合化”现象。</p><p>&emsp;&emsp;HNC用语义网络表达概念，其首要目标和价值在于<strong>给出概念关联性知识和联想脉络的线索，而不是给出概念的精确表示</strong>。自然语言理解的中心任务是解模糊，对自然语言词汇的多义选一处理是人类理解自然语言过程中最频繁、最基本的操作。<strong>语义层次网络符号的构造方式把最频繁、最基本的语义距离计算变成了层次符号的简单逐层比较，这是HNC用语义网络层次符号表达概念的基本出发点</strong>。</p><h2 id="3-语言知识库与本体论"><a href="#3-语言知识库与本体论" class="headerlink" title="3. 语言知识库与本体论"></a>3. 语言知识库与本体论</h2><p>&emsp;&emsp;本体(ontology)的的核心概念是<strong>知识共享</strong>，通过减少概念和术语上的歧义，建立一个统一的框架或规范模型，使得来自不同背景、持不同观点和目的的人员之间的理解和交流，以及不同系统之间的互操作或数据传输称为可能，并保持语义上的一致。</p><p>&emsp;&emsp;在一个领域中，本体构成了该领域任意知识表达系统的核心，而领域概念要通过领域中必用的一些词项来表达，这些称为术语(terminology)的领域词项，是领域的基本知识和信息的承载单位。本体可分为三个层次：上位本体(upper ontology)、领域本体(domain ontology)和面向应用的本体(application-oriented ontology)。上位本体是跨领域可复用的通用本体。领域本体有时也称为中位本体(mid-level ontology)，用于描述某个特定领域里最广泛使用的概念和关系。面向应用的本体则是为了某个应用而定制的本体知识库。</p><p>&emsp;&emsp;大多数上位本体采用自顶向下的方法经人工构建。目前被广泛使用的一个上位本体是建议上层共用知识本体(suggested upper merged ontology, SUMO)，该本体将几个公开可用本体的内容融合为一个具有一致性结构和广泛性的本体，它不仅包括概念的分类，也包括了可用于推导的公理和逻辑推断。</p><p>&emsp;&emsp;自动本体构建主要由两部分构成。第一部分用于确定领域中的概念集合。由于术语是概念的文字表征，所以<strong>概念的发现常常是通过术语发现(term discovery)来完成的</strong>。术语发现则从包括互联网文本、百科全书等不同领域资源中抽取得来。第二部分是关系发现(relationship discovery)，用以识别和提取概念之间的关系。</p><p>本体构建与语言知识库建设之间的关系：</p><ul><li>本体理论对于语言知识库的建设具有一定的借鉴意义。</li><li>本体的构建过程又离不开NLP技术。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP基础 </tag>
            
            <tag> 《统计自然语言处理》 </tag>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《统计自然语言处理》笔记-CH03-形式语言与自动机</title>
      <link href="/2020/02/09/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH03-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/"/>
      <url>/2020/02/09/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH03-%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="1-形式语言"><a href="#1-形式语言" class="headerlink" title="1. 形式语言"></a>1. 形式语言</h2><h3 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 概述"></a>1.1 概述</h3><p>&emsp;&emsp;乔姆斯基把语言定义为：<strong>按照一定规律构成的句子和符号串的有限或无限的集合</strong>。一般地，描述一种语言可以有三种途径：</p><ul><li><strong>穷举法</strong>：把语言中的所有句子枚举出来(仅适合句子数目有限的语言)。</li><li><strong>文法(产生式系统)描述</strong>：语言中的每个句子用严格定义的规则来构造，利用规则生成语言中合法的句子。</li><li><strong>自动机法</strong>：通过对输入的句子进行合法性检查，区分句子是否属于该语言。</li></ul><p>&emsp;&emsp;文法用来精确地描述语言及其结构，自动机则是用来机械地刻画对输入字符串的识别过程。<strong>用文法来定义语言的优点是：由文法给予语言中的句子以结构，各成分之间的结构关系清楚明了。但是，如果要直接用这些规则来确定一个字符串是否属于这套规则所定义的语言并不十分明确。而自动机来识别一个字符串是否属于该语言则相对简单，但自动机很难描述语言的结构</strong>。所以NLP中的识别和分析算法，大多兼取两者之长。</p><h3 id="1-2-形式语法的定义"><a href="#1-2-形式语法的定义" class="headerlink" title="1.2 形式语法的定义"></a>1.2 形式语法的定义</h3><ul><li><strong>形式语法</strong>：形式语法是一个四元组$G = (N, \sum, P, S)$，其中，$N$是非终结符的有限集合(有时也称变量集或句法种类集)；$\sum$是终结符号的有限集合，$N \cap \sum = \varnothing; V = N \cup \sum$称为总词汇表；$P$是一组重写规则的有限集合：$P = { \alpha \rightarrow \beta }$，其中$\alpha, \beta$是由$V$中元素构成的串，但$\alpha$中至少应含有一个非终结符号；$S \in N$称为句子符或初始符。</li><li><strong>推导</strong>：设$G = (N, \sum, P, S)$是一个文法，在$(N \cup \sum)^* $上定义关系$\Rightarrow_G$(直接派生或推导)为：如果$\alpha \beta \gamma$是$(N \cup \sum)^* $中的字符串，且$\beta \rightarrow \delta$是$P$中的一个产生式，那么$\alpha \beta \gamma \Rightarrow_G \alpha \delta \gamma$。如果每步推导只改写最左边的那个非终结符，这种推导称为<strong>最左推导</strong>。反之，如果每次都只改写最右边的非终结符，则为<strong>最右推导</strong>。最右推导又称规范推导。</li><li><strong>句子</strong>：文法$G = (N, \sum, P, S)$的句子形式(句型)通过如下递归方式定义：(1)$S$是一个句子形式；(2)如果$\gamma \beta \alpha$是一个句子形式，且$\beta \rightarrow \delta$是$P$中的产生式，那么$\gamma \delta \alpha$也是一个句子形式。</li></ul><h3 id="1-3-形式语法的类型"><a href="#1-3-形式语法的类型" class="headerlink" title="1.3 形式语法的类型"></a>1.3 形式语法的类型</h3><p>&emsp;&emsp;在乔姆斯基的语法理论中，文法被划分为4种类型：3型文法、2型文法、1型文法和0型文法，分别称为正则文法、上下文无关文法、上下文相关文法和无约束文法。</p><ul><li><strong>正则文法</strong>：如果文法$G$的规则集$P$中所有规则均满足如下形式：$A \rightarrow Bx$，或$A \rightarrow x$，其中$A, B \in N, x \in \sum$，则称该文法$G$为正则文法，或称3型文法。在这种书写格式中，由于规则右部的非终结符号(如果有的话)出现在最左边，所以这种形式的正则文法又称<strong>左线性正则文法</strong>。类似地，如果一正则文法所有含非终结符的规则形式为$A \rightarrow xB$，则该文法称为<strong>右线性文法</strong>。</li><li><strong>上下文无关文法(context-free grammar, CFG)</strong>：如果文法$G$的规则集$P$中所有规则均满足如下形式：$A \rightarrow \alpha, A \in N, \alpha \in (N \cup \sum)^* $，则称该文法为上下文无关文法，或称2型文法。2型文法比3型文法少了一层限制，其规则右端的格式没有约束，也就是说，规则左部的非终结符可以改写成任何形式。</li><li><strong>上下文有关文法(context-sensitive grammar, CSG)</strong>：如果文法$G$的规则集$P$中所有规则均满足如下形式：$\alpha A \beta \rightarrow \alpha \gamma \beta, A \in N, \alpha, \beta, \gamma \in (N \cup \sum)^* $，且$\gamma$至少包含一个字符，则称该文法为上下文有关文法，或称1型文法。从上述定义可以看出，<strong>字符串$\alpha A \beta$中的$A$被改写为$\gamma$时需要有上下文语境$\alpha , \beta$</strong>，这体现了上下文相关的含义。当然$\alpha, \beta$可以为空字符$\epsilon$，如果两者同时为空时，1型文法变成了2型文法，也就是说<strong>2型文法是1型文法的特例</strong>。</li><li><strong>无约束文法</strong>：如果文法$G$的规则集$P$中所有规则均满足如下形式：$\alpha \rightarrow \beta, \alpha \in (N \cup \sum)^+, \beta \in (N \cup \sum)^* $，则称该文法为无约束文法或无限制重写系统，也称0型文法。不难看出，<strong>从0型文法到3型文法，对规则的约束越来越多</strong>。</li></ul><h3 id="1-4-CFG识别句子的派生树表示"><a href="#1-4-CFG识别句子的派生树表示" class="headerlink" title="1.4 CFG识别句子的派生树表示"></a>1.4 CFG识别句子的派生树表示</h3><p>&emsp;&emsp;一个上下文无关文法$G = (N, \sum, P, S)$产生句子的过程可以由派生树表示。派生树也称语法树(syntactic tree)，或分析书(parsing tree)、推导树等。派生树的构造步骤如下：</p><ol><li>对于任意$x \in N \cup \sum$，给一个标记作为结点，令文法的初始符号$S$作为树的根节点；</li><li>如果一个节点的标记为$A$，且至少有一个除它本身以外的后裔，那么$A \in N$；</li><li>如果一个节点的标记为$A$，它的$k(k &gt; 0)$个直接后裔结点按从左到右的顺序依次标记为$A_1, A_2, …, A_k$，则$A \rightarrow A_1, A_2, …, A_k$一定是$P$中的一个产生式。</li></ol><h2 id="2-自动机理论"><a href="#2-自动机理论" class="headerlink" title="2. 自动机理论"></a>2. 自动机理论</h2><p>&emsp;&emsp;自动机分为以下4种类型：有限自动机(finite automata, FA)、下推自动机(push-down automata, PDA)、线性界限自动机(linear-bounded automata)和图灵机(Turing machine)。</p><h3 id="2-1-有限自动机"><a href="#2-1-有限自动机" class="headerlink" title="2.1 有限自动机"></a>2.1 有限自动机</h3><p>&emsp;&emsp;有限自动机又分为确定性有限自动机(definite automata, DFA)和不确定性有限自动机(non-definite automata, NFA)两种。</p><ul><li><strong>DFA</strong>：$M = (\sum, Q, \delta, q_0, F)$是一个五元组，其中$\sum$是输入符号的有穷集合；$Q$是状态的有限集合；$q_0 \in Q$是初始状态；$F$是终止状态集合，$F \subseteq Q; \delta$是$Q \times \sum$到$Q$(下一个状态)的映射，它支配着有限状态控制的行为，也称状态转移函数。</li><li><strong>DFA接受的语言</strong>：如果一个句子$x$对于有限自动机$M$有：$\delta(q_0, x) = p, p \in F$，那么称句子$x$被$M$所接受。</li><li><strong>NFA</strong>：$M = (\sum, Q, \delta, q_0, F)$是一个五元组，其中$\sum$是输入符号的有穷集合；$Q$是状态的有限集合；$q_0 \in Q$是初始状态；$F$是终止状态集合，$F \subseteq Q; \delta$是$Q \times \sum$到$Q$的幂集$2^Q$的映射。NFA与DFA的区别在于：<strong>在NFA中$\delta(q_0, a)$是一个状态集合，而在DFA中$\delta(q_0, a)$是一个状态</strong>。</li><li><strong>NFA接受的语言</strong>：如果存在一个状态$p, p \in \delta(q_0, x)$且$p \in F$，则称句子$x$被NFA $M$所接受。</li><li>定理3-1：设$L$是被NFA所接受的语言，则存在一个DFA，它能够接受$L$。</li></ul><h3 id="2-2-正则文法与自动机的关系"><a href="#2-2-正则文法与自动机的关系" class="headerlink" title="2.2 正则文法与自动机的关系"></a>2.2 正则文法与自动机的关系</h3><ul><li><strong>定理3-2</strong>：若$G = (V_N, V_T, P, S)$是一个正则文法，则存在一个FA $M = (\sum, Q, \delta, q_0, F)$，使得$T(M) = L(G)$。</li></ul><p>&emsp;&emsp;根据这个定理，可以用以下方法<strong>由给定的正则文法$G$构造FA $M$</strong>。具体步骤如下：</p><ol><li>令$\sum = V_T, Q= V_N \cup {T}, q_0 = S$，其中$T$是一个新增加的非终结符；</li><li>如果$P$中有产生式$S \rightarrow \epsilon$，则$F = {S, T}$，否则$F = {T}$；</li><li>如果$P$中有产生式$B \rightarrow a, B \in V_N, a \in V_T$，则$T \in \delta(B, a)$；</li><li>如果$P$中有产生式$B \rightarrow aC, B,C \in V_N, a \in V_T$，则$C \in \delta(B, a)$；</li><li>对于每一个$a \in V_T$，有$\delta(T, a) = \varnothing$。</li></ol><ul><li><strong>定理3-3</strong>：若$M = (\sum, Q, \delta, q_0, F)$是一个有限自动机，则存在一个正则文法$G = (V_N, V_T, P, S)$，使得$L(G) = T(M)$。由FA $M$构造$G$的一般步骤如下：</li></ul><ol><li>令$V_N = Q, V_T = \sum, S = q_0$；</li><li>如果$C \in \delta(B, a), B, C \in Q, a \in \sum$，则在$P$中有产生式$B \rightarrow aC$；</li><li>如果$C \in \delta(B, a), C \in F$，则在$P$中有产生式$B \rightarrow a$。</li></ol><h3 id="2-3-上下文无关文法与下推自动机"><a href="#2-3-上下文无关文法与下推自动机" class="headerlink" title="2.3 上下文无关文法与下推自动机"></a>2.3 上下文无关文法与下推自动机</h3><ul><li><strong>定义</strong>：PDA可以表达成一个七元组：$M =(\sum, Q, \Gamma, \delta, q_0, Z_0, F)$。其中$\sum $是输入符号的有穷集合；$Q$是状态的有限集合；$\Gamma$是下推存储器符号的有穷集合；$q_0 \in Q$是初始状态；$Z_0 \in \Gamma$是最初出现在下推存储器顶端的开始符号；$F \subseteq Q$是终止状态集合；$\delta$是从$Q \times (\sum \cup {\epsilon} \times \Gamma)$到$Q \times \Gamma^* $的子集的映射。</li></ul><p>&emsp;&emsp;下推自动机PDA可以看成是一个带有附加下推存储器的有限自动机，下推存储器是一个栈(stack)，其原理如下图3-7所示：</p><div align=center><img src="http://tva1.sinaimg.cn/large/006Ahf5Fly1gbqdfi4w81j30dc06m0th.jpg"></div><p>对于PDA，判断一种语言(或一个句子)是否被PDA接受的标准有两种：</p><ol><li>终止状态接受标准：对于输入句子$x$，如果PDA从初始状态$q_0$开始转换到终止状态$q$时，$x$正好被读完，则认为$x$被PDA $M$所接受，而<strong>不管此时下推存储器的内容如何</strong>。</li><li>空存储器接受标准：对于给定的输入句子$x$，当输入头指向$x$的末端时，如果下推存储器为空，则认为$x$被PDA $M$所接受，而<strong>不管此时PDA的状态$q$是否在终止状态集中</strong>。</li></ol><p>PDA与上下文无关文法的关系如下：</p><ul><li>设$L(G)$为上下文无关文法语言，则存在一个PDA，使得PDA以空存储器标准所接受的语言$N(M) = L(G)$。</li><li>设$N(M)$为由PDA以空存储器标准所接受的语言，则存在一个CFG使得$L(G) = N(M)$。</li></ul><h3 id="2-4-图灵机"><a href="#2-4-图灵机" class="headerlink" title="2.4 图灵机"></a>2.4 图灵机</h3><ul><li><strong>定义</strong>：一个图灵机$T$可以表示成一个六元组：$T = (\sum, Q, \Gamma, \delta, q_0, F)$。其中$\sum $是输入/输出带上符号的有穷集合，不包括空白符号$B$；$Q$是状态的有限集合；$\Gamma$是输入符号的有穷集合，包含空符号$B, \sum \subseteq \Gamma, \Gamma = \sum \cup {B}$；$q_0 \in Q$是初始状态；$F \subseteq Q$是终止状态集合；$\delta$是从$Q \times \Gamma$到$Q \times (\Gamma- {B}) \times {R, L, S}$子集的映射。其中$R, L, S$分别代表右移一格、左移一格和停止不动。</li></ul><p>&emsp;&emsp;图灵机与有限自动机的区别在于<strong>图灵机可以通过其读写头改变输入带上的字符</strong>，而有限自动机不能做到这一点。图灵机和0型文法的关系如下：</p><ul><li>如果$L$是一个由0型文法产生的语言，则$L$可以被一个图灵机所接受。</li><li>如果$L$可被一个图灵机所接受，则$L$是一个由0型文法产生的语言。</li></ul><h3 id="2-5-线性界限自动机"><a href="#2-5-线性界限自动机" class="headerlink" title="2.5 线性界限自动机"></a>2.5 线性界限自动机</h3><ul><li><strong>定义</strong>：一个线性界限自动机$M$可以表达成一个六元组：$M = (\sum, Q, \Gamma, \delta, q_0, F)$。其中$\sum $是输入/输出带上符号的有穷集合，$\sum \subseteq \Gamma$；$Q$是状态的有限集合；$\Gamma$是输入/输出带上符号的有穷集合；$q_0 \in Q$是初始状态；$F \subseteq Q$是终止状态集合；$\delta$是从$Q \times \Gamma$到$Q \times \Gamma \times {R, L}$子集的映射。$\sum$包括两个特殊符号#和$，分别表示输入链的左端和右端结束标志。</li></ul><p>&emsp;&emsp;线性界限自动机是一个确定的单带图灵机，其读/写头不能超越原输入带上字符串的初始和终止位置，及线性界限自动机的存储空间被输入符号串的长度所限制。其与上下文相关文法的关系如下：</p><ul><li>如果$L$是一个上下文相关语言，则$L$由一个不确定的线性界限自动机所接受。反之，如果$L$被一个线性界限自动机所接受，则$L$是上下文相关语言。</li></ul><hr><p>&emsp;&emsp;归纳起来，各类自动机之间的主要区别是它们能够使用的信息存储空间的差异：有限状态自动机只能用状态存储信息；下推自动机除了使用状态外，还可以用下推存储器(栈)；线性界限自动机可以利用状态和输入/输出带本身，因为输入/输出带没有”先进后出”的限制，因此其功能大于栈；而图灵机的存储空间没有任何限制。<br>&emsp;&emsp;从识别语言的能力上看，有限自动机等价于正则文法；下推自动机等价于上下文无关文法；线性界限自动机等价于上下文有关文法；图灵机等价于无约束文法。</p><hr><h2 id="3-有限自动机在NLP中的应用"><a href="#3-有限自动机在NLP中的应用" class="headerlink" title="3. 有限自动机在NLP中的应用"></a>3. 有限自动机在NLP中的应用</h2><ul><li>单词拼写检查</li><li>单词形态分析</li><li>词性消歧</li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP基础 </tag>
            
            <tag> 《统计自然语言处理》 </tag>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯决策理论</title>
      <link href="/2020/02/07/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA/"/>
      <url>/2020/02/07/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp; <strong>贝叶斯决策理论(Bayesian decision theory)</strong> 是统计方法处理模式分类问题的基本理论之一。在之前的学习过程中接触过该理论，但一直没理解其思想。本文试图探讨以下问题：</p><ul><li>贝叶斯决策理论是什么？有什么用处？</li><li>贝叶斯决策理论中”先验概率”，”后验概率”，”贝叶斯公式”是什么？</li><li>如果说贝叶斯决策理论是最优的，为什么还需要其他算法？</li></ul><hr><p>&emsp;&emsp;机器学习(Machine Learning)可以理解为<strong>通过观测数据$(X)$推测结果$(y)$，同时学习推断规则$(c)$，使得推断结果$\hat y = c(X)$与真实值$y$的误差尽可能小</strong>。举个例子，我们现在有一批西瓜(为简单起见，现假设仅通过根蒂就能判断西瓜的好坏)，并观察到了它们的根蒂形状$(X)$，就可以通过推断规则$(c)$来估计西瓜是好瓜还是坏瓜$(y)$。全文我们都会举西瓜的例子，我们能观察到的西瓜根蒂$X$(蜷缩、硬挺、稍蜷)，西瓜的好坏$y$：好瓜$(y=1)$、坏瓜$(y=0)$。</p><h3 id="1-在没有其他信息时，我们如何推断？"><a href="#1-在没有其他信息时，我们如何推断？" class="headerlink" title="1. 在没有其他信息时，我们如何推断？"></a>1. 在没有其他信息时，我们如何推断？</h3><ul><li>假设我们第一次接触判断西瓜好坏的问题，什么都不知道。此时只能预测好瓜和坏瓜的概率都是0.5。</li><li>假设我们虽然第一次接触判断西瓜好坏的问题，但神告诉我们这种西瓜是好瓜的概率$P(1) = 0.7$，是坏瓜的概率$P(0) = 0.3$。显然，此时我们倾向于预测西瓜是好瓜，且预测错误的概率是$1-P(1) = P(0) = 0.3$。</li></ul><p>&emsp;&emsp;此处的$P(1)=0.7$和$P(0)=0.3$就叫做<strong>先验概率(prior probability)</strong>，指的是在观测前我们就已知结果的概率分布$P(y)$。</p><h3 id="2-当有了更多信息时，该如何利用它们？"><a href="#2-当有了更多信息时，该如何利用它们？" class="headerlink" title="2. 当有了更多信息时，该如何利用它们？"></a>2. 当有了更多信息时，该如何利用它们？</h3><p>&emsp;&emsp;显然，前文的推测方式是很不准确的，因为没有考虑到西瓜的属性。而现实情况是我们往往可以观察到西瓜的一些属性，并非完全一无所知。因此，我们尝试回答：在观察到西瓜的根蒂形状后，它是好瓜的概率是多少？对应的数学表达式就是$P(y=1 \mid X)$。相应的，是坏瓜的数学表达式为$P(y=0 \mid X)$。</p><p>&emsp;&emsp;此处的$P(y=1 \mid X)$和$P(y=0 \mid X)$就叫做<strong>后验概率(posterior probability)</strong>，指的是在观测到$X$后我们对结果$y$的估计。可以简单地理解为，<strong>后验概率是对先验概率的一种”更加细致准确的刻画”，因为此时我们观测到了$X$，有了额外的信息，所以后验概率比先验概率更有意义</strong>。大部分机器学习模型尝试得到的就是后验概率。</p><h3 id="3-如何求得后验概率？"><a href="#3-如何求得后验概率？" class="headerlink" title="3. 如何求得后验概率？"></a>3. 如何求得后验概率？</h3><p>&emsp;&emsp;我们无法直接获得后验概率，但<strong>可以通过贝叶斯公式计算得到</strong>。根据条件概率易知：<br>$$P(A \mid B) = \frac{P(AB)}{P(B)} \Rightarrow P(AB) = P(A \mid B) \cdot P(B) \tag{1}$$ $$P(B \mid A) = \frac{P(BA)}{P(A)} \Rightarrow P(BA) = P(B \mid A) \cdot P(A) \tag{2}$$</p><p>因为$P(AB) = P(BA)$，所以$P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A)$，进而推出了大名鼎鼎的<strong>贝叶斯公式</strong>：$$P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)} \tag{3}$$ 将其带入到上文中的后验概率中，可求得：$$P(y=1 \mid X) = \frac{P(X \mid y=1) \cdot P(y=1)}{P(X)} \tag{4}$$ $$P(y=0 \mid X) = \frac{P(X \mid y=0)\cdot P(y=0)}{P(X)} \tag{5}$$ 我们发现，在通过贝叶斯公式变形后，后验概率变得可以计算了。<strong>贝叶斯决策的预测值是选取后验概率最大的结果</strong>。</p><h3 id="4-为什么贝叶斯决策理论是”最优的”？"><a href="#4-为什么贝叶斯决策理论是”最优的”？" class="headerlink" title="4. 为什么贝叶斯决策理论是”最优的”？"></a>4. 为什么贝叶斯决策理论是”最优的”？</h3><p>&emsp;&emsp;因为$P(y=0 \mid X) = 1- P(y=1 \mid X)$，<strong>只要观察到$X$，并选择后验概率最大的结果，就可以最小化预测的错误概率。这个结论对所有的观测值$X$均成立，所以才叫做”最优”</strong>。</p><h3 id="5-贝叶斯决策理论有什么用？"><a href="#5-贝叶斯决策理论有什么用？" class="headerlink" title="5. 贝叶斯决策理论有什么用？"></a>5. 贝叶斯决策理论有什么用？</h3><p>&emsp;&emsp;我们定义最小误分概率(最优情况下):$$R^* = \sum_X P(X) \cdot min(P(y=0 \mid X), P(y=1 \mid X)) \tag{6}$$ 易知$\sum_X P(X)=1$且$0 \leq P(X_i) \leq 1$。在二分类的情况下$0 \leq min(P(y=0 \mid X), P(y=1 \mid X)) \leq 0.5$，因此最小误分概率$0 \leq R^* \leq 0.5$。<br>&emsp;&emsp;举个例子，假设在所有的观测$X$下，$min(P(y=0 \mid X), P(y=1 \mid X)) \leq 0.2$成立，那么我们可以知道这个决策的最大误分概率就是$0.2$，即<strong>不存在任何其他决策规则会得到低于$0.2$的误分概率</strong>。所以贝叶斯决策理论可以用来<strong>估算理论上的计算边界(Bound)，尤其是误差的上界</strong>。</p><h3 id="6-如果说贝叶斯决策理论是最优的，为什么还需要其他算法？"><a href="#6-如果说贝叶斯决策理论是最优的，为什么还需要其他算法？" class="headerlink" title="6. 如果说贝叶斯决策理论是最优的，为什么还需要其他算法？"></a>6. 如果说贝叶斯决策理论是最优的，为什么还需要其他算法？</h3><p>&emsp;&emsp;既然贝叶斯决策理论是最优的，为什么不能盖过深度学习的锋芒呢？其根本原因在于贝叶斯决策中假设比较强，实际操作起来并不容易：</p><ul><li>无法获得真实的先验概率等其他计算要素，需要通过统计学的方法进行估计，存在一定的误差。</li><li>计算所需要的概率往往是不可能的，会面临维度灾难(the curse of dimensionality)。在条件概率$P(y=0 \mid X)$和$P(y=1 \mid X)$中，要考虑观察的特征不止一个，可能还有其他特征。而且实际计算时往往面临很多数值稳定性问题，所以朴素贝叶斯才会介绍平滑(smoothing)来解决一些特例。</li></ul><p>&emsp;&emsp;综上所述，<strong>直接利用贝叶斯决策理论进行后仰概率估算是不大实用的，但对于理论上计算边界，尤其是误差的上界比较有意义</strong>。</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://www.zhihu.com/question/27670909/answer/378129817" target="_blank" rel="noopener">如何简单理解贝叶斯决策理论?</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> ML理论知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 理论知识 </tag>
            
            <tag> Machine Learning基础 </tag>
            
            <tag> 贝叶斯决策理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《统计自然语言处理》笔记-CH02-预备知识</title>
      <link href="/2020/02/07/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH02-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/"/>
      <url>/2020/02/07/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH02-%E9%A2%84%E5%A4%87%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>本章将对概率论、信息论和支持向量机等有关概念做简要的介绍。</p><h2 id="1-概率论基本概念"><a href="#1-概率论基本概念" class="headerlink" title="1. 概率论基本概念"></a>1. 概率论基本概念</h2><h3 id="1-1-概率"><a href="#1-1-概率" class="headerlink" title="1.1 概率"></a>1.1 概率</h3><p>&emsp;&emsp;概率(probability)是从随机实验中的事件到实数域的映射函数，用以表示事件发生的可能性。用$P(A)$表示事件A发生的概率，$\Omega$是实验的样本空间，则概率函数满足以下三条定理：</p><ul><li>非负性：$P(A)\geq 0$</li><li>规范性：$P(\Omega) = 1$</li><li>可列可加性：对于可列无穷多个事件$A_1$，$A_2$，…，如果事件<strong>两两不相容</strong>，即对于任意的$i$和$j$，$(i \neq j)$，事件$A_i$和$A_j$ 不相交 $(A_i \cap A_j = \varnothing )$，则有：$$P(\bigcup_{i=0}^\infty A_i) = \sum_{i=0}^\infty P(A_i) \tag{2-1}$$</li></ul><h3 id="1-2-最大似然估计"><a href="#1-2-最大似然估计" class="headerlink" title="1.2 最大似然估计"></a>1.2 最大似然估计</h3><p>&emsp;&emsp;如果${ s_i, s_2, …, s_n }$是一个试验的样本空间，在相同的情况下重复试验$N$次，观察到样本$s_k (1 \leq k \leq n)$的次数为$n_k(s_k)$，那么$s_k$在这$N$次试验中的相对频率为：$$q_N (s_k) = \frac{n_k(s_k)}{N} \tag{2-2}$$<br>由于$\sum_{k=1}^n n_N(s_k) = N$，因此$\sum_{k=1}^n q_n(s_k) = 1$。</p><p>&emsp;&emsp;当$N$越来越大时，相对频率$q_N(s_k)$就越来越接近$s_K$的概率$P(s_k)$。事实上，$$\lim_{N \rightarrow \infty} q_N(s_k) = P(s_k) \tag{2-3}$$</p><p>因此，通常用相对频率作为概率的估计值。这种估计概率值的方法称为<strong>最大似然估计(maximum likelihood estimation, MLE)</strong>。</p><h3 id="1-3-条件概率"><a href="#1-3-条件概率" class="headerlink" title="1.3 条件概率"></a>1.3 条件概率</h3><ul><li>(1)非负性：$P(A \mid B) \geq 0$</li><li>(2)规范性：$P(\Omega \mid B) = 1$</li><li>(3)可列可加性：如果事件$A_1, A_2, …$两两互不相容，则$$P(\sum_{i=1}^{\infty} A_i \mid B) = \sum_{i=1}^{\infty} P(A_i \mid B) \tag{2-4}$$</li><li>如果$A_i, A_j$<strong>条件独立</strong>，当且仅当$$P(A_i, A_j \mid B) = P(A_i \mid B) \times P(A_j \mid B) \tag{2-5}$$</li></ul><h3 id="1-4-贝叶斯法则"><a href="#1-4-贝叶斯法则" class="headerlink" title="1.4 贝叶斯法则"></a>1.4 贝叶斯法则</h3><p>&emsp;&emsp;假设$B$是样本空间$\Omega$的一个划分，即$\sum_i B_i = \Omega$。如果$A \subseteq \bigcup_i B_i$，并且$B_i$互不相交，那么$A = \sum_{i=1} B_iA$，于是$P(A) = \sum_{i=1} P(B_iA)$。由乘法定理可得$$P(A) = \sum_i P(A \mid B_i)P(B_i) \tag{2-6}$$</p><p>公式(2-6)称为<strong>全概率公式</strong>。</p><p>&emsp;&emsp;<strong>贝叶斯法则</strong>，或称<strong>贝叶斯理论(Bayesian theorem)</strong>，是条件概率计算的重要依据。假设$A$为样本空间$\Omega$的事件，$B$是$\Omega$的一个划分，如果$A \subseteq \bigcup_{i=1}^n B_i, P(A) &gt; 0$，并且$i \neq j, B_i \cap B_j = \varnothing, P(B_i) &gt; 0(i=1,2,…,n)$，则$$P(B_j \mid A) = \frac{P(A \mid B_j)P(B_j)}{P(A)} = \frac{P(A \mid B_j)P(B_j)}{\sum_{i=1}^n P(A \mid B_i)P(B_i)} \tag{2-7}$$</p><h3 id="1-5-二项式分布"><a href="#1-5-二项式分布" class="headerlink" title="1.5 二项式分布"></a>1.5 二项式分布</h3><p>&emsp;&emsp;在NLP中，一般以句子为处理单位。为了简化问题的复杂性，通常<strong>假设一个句子的出现独立于它前面的其他语句</strong>，句子的概率分布近似地被认为符合二项式分布。</p><h3 id="1-6-贝叶斯决策理论"><a href="#1-6-贝叶斯决策理论" class="headerlink" title="1.6 贝叶斯决策理论"></a>1.6 贝叶斯决策理论</h3><p>&emsp;&emsp;贝叶斯决策理论(Bayesian decision theory)是统计方法处理模式分类问题的基本理论之一。假设研究的分类问题有$c$个类别，各类别的状态用$\omega_i (i=1, 2, …, c)$表示；对应于各个类别$\omega_i$出现的先验概率为$P(\omega_i)$；在特征空间已经观察到某一向量$x, x = [x_1, x_2, …, x_d]^T$是$d$维特征空间上的某一点，且条件概率密度函数$P(x \mid \omega_i)$是已知的。那么利用贝叶斯公式我们可求得后验概率$P(\omega_i \mid x)$如下：$$P(\omega_i \mid x) = \frac{P(x \mid \omega_i) \cdot P(\omega_i)}{\sum_{j=1}^c P(x \mid \omega_j) \cdot P(\omega_j)} \tag{2-8}$$ 基于最小错误率的贝叶斯决策规则为：</p><ol><li>如果$P(\omega_i \mid x) = max_{j=1, 2, …, c} P(\omega_j \mid x)$，那么$x \in \omega_i$</li></ol><p>或者说：</p><ol start="2"><li>如果$P(x \mid \omega_i)P(\omega_i) = max_{j=1, 2, …, c} P(x \mid \omega_j)P(\omega_j)$，则$x \in \omega_i$</li></ol><p>如果类别只有两类时，即$c=2$，则有：</p><ol start="3"><li>如果$l(x) = \frac{P(x \mid \omega_1)}{P(x \mid \omega_2)} &gt; \frac{P(\omega_2)}{P(\omega_1)}$，则$x \in \omega_1$否则$x \in \omega_2$。其中$l(x)$为似然比(likelihood ratio)，而$\frac{P(\omega_2)}{P(\omega_1)}$称为似然比阈值(threshold)。</li></ol><h2 id="2-信息论基本概念"><a href="#2-信息论基本概念" class="headerlink" title="2. 信息论基本概念"></a>2. 信息论基本概念</h2><h3 id="2-1-熵"><a href="#2-1-熵" class="headerlink" title="2.1 熵"></a>2.1 熵</h3><p>&emsp;&emsp;熵(entropy)是信息论的基本概念。如果$X$是一个离散型随机变量，取值空间为$R$，其概率分布为$P(x) = P(X=x), x \in R$。那么$X$的熵$H(X)$定义为：$$H(X) = - \sum_{x \in R} P(x)log_2 P(x) \tag{2-9}$$ 其中，约定$0log 0 = 0$。由于在公式(2-9)中对数以2为底，该公式定义的熵的单位为二进制位(比特)。通常将$log_2 P(x)$简写为$log P(x)$。</p><p>&emsp;&emsp;熵又称为自信息(self-information)，可视为描述一个随机变量的不确定性的数量。它表示信源$X$每发一个符号所提供的平均信息量。<strong>一个随机变量的熵越大，它的不确定性就越大，正确估计其值的可能性就越小</strong>。在已知部分知识的前提下，符合已知知识的概率分布可能有多个，但使熵值最大的概率分布最真实地反映了事件的分布情况，<strong>最大熵概念</strong>被广泛地应用于NLP中。</p><h3 id="2-2-联合熵和条件熵"><a href="#2-2-联合熵和条件熵" class="headerlink" title="2.2 联合熵和条件熵"></a>2.2 联合熵和条件熵</h3><p>&emsp;&emsp;如果$X, Y$是一对离散型随机变量$X, Y \sim P(x, y), X, Y$的联合熵(joint entropy)$H(X, Y)$定义为：$$H(X, Y) = - \sum_{x \in X} \sum_{y \in Y} P(x, y)logP(x, y) \tag{2-10}$$ 给定随机变量$X$的情况下，随机变量$Y$的条件熵(conditional entropy)定义为：$$H(Y \mid X) = \sum_{x \in X} P(x)H(Y \mid X = x) = - \sum_{x \in X} \sum_{y \in Y} P(x, y)log P(y \mid x) \tag{2-11}$$ 将式(2-10)中的联合概率$log P(x, y)$展开，可得：$$H(X, Y) = - \sum_{x \in X} \sum_{y \in Y} P(x, y) log [ P(x) P(y \mid x)] = H(X) + H(Y \mid X) \tag{2-12}$$ 我们称式(2-12)为<strong>熵的连锁规则(chain rule for entropy)</strong>。推广到一般情况，有：$$H(X_1, X_2, …, X_n) = H(X_1) + H(X_2 \mid X_1) + … + H(X_n \mid X_1, X_2, …, X_{n-1}) \tag{2-13}$$</p><h3 id="2-3-互信息"><a href="#2-3-互信息" class="headerlink" title="2.3 互信息"></a>2.3 互信息</h3><p>&emsp;&emsp;根据熵的连锁规则，有：$$H(X, Y) = H(X) + H(Y \mid X) = H(Y) + H(X \mid Y)$$ 因此，$H(X) - H(X \mid Y) = H(Y) - H(X \mid Y)$。<strong>这个差叫做$X$和$Y$的互信息(mutual information, MI)，记作$I(X;Y)$</strong>。其反映的是在知道了$Y$的值以后$X$的不确定性的减少量。可以理解为$Y$的值透露了多少关于$X$的信息量。</p><p>&emsp;&emsp;互信息体现了两个变量之间的依赖程度：如果$I(X;Y) &gt;&gt; 0$表明$X, Y$是高度相关的；如果$I(X;Y) = 0$，表明$X, Y$是相互独立的；如果$I(X;Y) &lt;&lt; 0$，表明$Y$的出现不但未使$X$的不确定性减小，反而增大了$X$的不确定性，常是不利的。平均互信息量是非负的。</p><h3 id="2-4-相对熵"><a href="#2-4-相对熵" class="headerlink" title="2.4  相对熵"></a>2.4  相对熵</h3><p>&emsp;&emsp;相对熵(relative entropy)又称Kullback-Leibler差异，或简称KL距离，是衡量相同事件空间里两个概率分布相对差距的测度。两个概率分布$p(x), q(x)$的相对熵定义为：$$D(p ||q) = \sum_{x \in X} p(x) log \frac{p(x)}{q(x)} \tag{2-14}$$ 该定义中约定$0 log (0/q) = 0, p log(p/0) = \infty$。显然，当两个随机分布完全相同时，其相对熵为0。当两个随机分布的差别增大时，其相对熵期望值也增大。</p><h3 id="2-5-交叉熵"><a href="#2-5-交叉熵" class="headerlink" title="2.5 交叉熵"></a>2.5 交叉熵</h3><p>&emsp;&emsp;交叉熵(cross entropy)的概念是用来衡量估计模型与真实概率分布之间的差异情况。如果一个随机变量$X \sim p(x), q(x)$为用于近似$p(x)$的概率分布，那么随机变量$X$和模型$q$之间的交叉熵定义为：$$H(X, q) = - \sum_x p(x) log q(x) \tag{2-15}$$</p><h3 id="2-6-困惑度"><a href="#2-6-困惑度" class="headerlink" title="2.6 困惑度"></a>2.6 困惑度</h3><p>&emsp;&emsp;在设计语言模型时，我们通常用困惑度(perplexity)来代替交叉熵衡量语言模型的好坏。给定语言$L$的样本$l_1^n = l_1 … l_n, L$的困惑度$PP_q$定义为：$$PP_q = 2^{H(L, q)} \approx 2^{- \frac{1}{n} log q(l_1^n)} = [ q(l_1^n) ]^{- \frac{1}{n}} \tag{2-16}$$ 同样，语言模型设计的任务就是寻找困惑度最小的模型，使其最接近真实语言的情况。</p><h3 id="2-7-噪声信道模型"><a href="#2-7-噪声信道模型" class="headerlink" title="2.7 噪声信道模型"></a>2.7 噪声信道模型</h3><p>&emsp;&emsp;噪声信道模型(noisy channel model)目标是优化噪声信道中信号传输的吞吐量和准确率，其<strong>基本假设是一个信道的输出以一定概率依赖于输入</strong>。<br>&emsp;&emsp;一般情况下，在信号传输的过程中都要进行双重性处理：一方面要对编码进行压缩，尽量消除所有的冗余；另一方面又要通过增加一定的可控冗余以保障输入信号经过噪声信道传输后可以很好地恢复原状。<br>&emsp;&emsp;信息论中另一个重要概念是信道容量(capacity)，其基本思想是用降低传输速率来换取高保真通信的可能性。其定义可以根据互信息给出：$$C = \max_{p(X)} I(X;Y) \tag{2-17}$$ 根据此定义，如果能够设计一个输入编码$X$，其概率分布为$p(X)$，使其输入与输出之间的互信息达到最大值，那么我们的设计就达到了信道的最大传输容量。从数学上将，式(2-17)所表示的信道容量$C$就是平均互信息量的最大值。</p><p>&emsp;&emsp;在NLP中不需要进行编码，一种自然语言的句子可以视为<strong>已编码</strong>的符号序列，但需要进行解码，使观察到的输出序列更接近于输入。因此可以用图2-4表示这个噪声信道模型。</p><div align=center><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gbp3bphazsj30ag02wt8z.jpg" width="250" height="80"></div><p>&emsp;&emsp;模拟信道模型，在NLP中，很多问题都可以归结为给定输出$O$(可能含有误传信息)的情况下，如何从所有的可能输入$I$中求解最有可能的那个，即求出使$p(I \mid O)$最大的$I$作为输入$\hat I$。根据贝叶斯公式，有$$\hat I = \argmax_I p(I \mid O) = \argmax_I \frac{p(I) p(O \mid I)}{p(O)} = \argmax_I p(I) p(O \mid I) \tag{2-18}$$ 式(2-18)中$p(I)$称为语言模型，是指在输入语言中”词”序列的概率分布；$p(O \mid I)$称为信道概率。</p><hr><h2 id="附："><a href="#附：" class="headerlink" title="附："></a>附：</h2><ul><li>Mathjax语法：<a href="https://qianwenma.cn/2018/05/17/mathjax-yu-fa-can-kao/" target="_blank" rel="noopener">Mathjax语法参考</a>，<a href="https://blog.csdn.net/ajacker/article/details/80301378" target="_blank" rel="noopener">Mathjax语法总结</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP基础 </tag>
            
            <tag> 《统计自然语言处理》 </tag>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《统计自然语言处理》笔记-CH01-绪论</title>
      <link href="/2020/02/06/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH01-%E7%BB%AA%E8%AE%BA/"/>
      <url>/2020/02/06/%E3%80%8A%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E3%80%8B%E7%AC%94%E8%AE%B0-CH01-%E7%BB%AA%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><h3 id="1-1-语言学与语音学"><a href="#1-1-语言学与语音学" class="headerlink" title="1.1 语言学与语音学"></a>1.1 语言学与语音学</h3><ul><li>语言学(linguistics)是指对语言的科学研究，根据语言学家的注意中心和和兴趣范围可分为<strong>历史语言学、共时语言学、一般语言学、理论语言学、描述语言学、对比语言学、结构语言学</strong>等。</li><li>语音学(phonetics)是研究人类发音特点，特别是语音发音特点，并提出各种语言描述、分类和转写方法的科学，一般有三个分支：<strong>发音语音学、声学语音学、听觉语音学</strong>。</li></ul><h3 id="1-2-自然语言处理"><a href="#1-2-自然语言处理" class="headerlink" title="1.2 自然语言处理"></a>1.2 自然语言处理</h3><ul><li>自然语言处理(NLP)是研究人与人交际中以及人与计算机交际中的语言问题的一门学科。自然语言处理要研制表示语言能力(linguistic competence)和语言应用(linguistic performance)的模型，建立计算框架来实现这样的语言模型，提出相应的方法来不断地完善这样的语言模型，根据这样的语言模型设计各种实用系统，并探讨这些实用系统的测评技术。</li><li>计算语言学(computational linguistics)是语言学的一个分支，用计算技术和概念来阐述语言学和语音学的问题。已开发的领域包括自然语言处理、言语合成、言语识别、自动翻译、语法的检测，以及许多需要统计分析的领域。</li></ul><h3 id="1-3-关于”理解”的标准"><a href="#1-3-关于”理解”的标准" class="headerlink" title="1.3 关于”理解”的标准"></a>1.3 关于”理解”的标准</h3><ul><li>图灵测试(Turing test)：测试人在一段规定的时间内，在无法看到反映来源的情况下，根据两个实体(被测试的计算机系统和另外一个人)对他提出的各种问题的反应来判断做出反应的是人还是计算机。通过一系列这样的测试，从计算机被误认为人的几率就可以测出计算机系统所具有的智能程度。</li></ul><p>&emsp;&emsp;在NLP领域，采用图灵实验来判断计算机是否”理解”了某种自然语言的具体准侧可以有很多，例如通过问答系统测试计算机系统是否能够正确地回答输入文本中的有关问题；通过文摘生成系统测试计算机系统是否有能力自动产生输入文本的摘要等。实际上，人们在NLP领域研究的任何一个应用系统都可以做图灵测试。<strong>按照人的标准对这些系统的输出结果进行评价，从而判断计算机系统是否达到了”理解”的效果</strong>。</p><h2 id="2-自然语言处理研究的内容和面临的困难"><a href="#2-自然语言处理研究的内容和面临的困难" class="headerlink" title="2. 自然语言处理研究的内容和面临的困难"></a>2. 自然语言处理研究的内容和面临的困难</h2><h3 id="2-1-自然语言处理研究的内容"><a href="#2-1-自然语言处理研究的内容" class="headerlink" title="2.1 自然语言处理研究的内容"></a>2.1 自然语言处理研究的内容</h3><ul><li>机器翻译：实现一种语言到另一种语言的自动翻译。</li><li>自动文摘：将原文档的主要内容和含义自动归纳、提炼出来，形成摘要或缩写。</li><li>信息检索：也称情报检索，就是利用计算机系统从海量文档中找到符合用户需要的相关文档。面向两种及以上语言的信息检索叫做跨语言检索。</li><li>文档分类：也称文本分类或信息分类，其目的是利用计算机系统对大量的文档按照一定的分类标准实现自动归类。近年来情感分类或称文本倾向性识别成为本领域的研究热点，情感分析已成为支撑舆情分析的基本技术。</li><li>问答系统：通过计算机系统对用户提出的问题的理解，利用自动推理等手段，在有关知识资源中自动求解答案并做出相应的回答。问答技术有时与语音技术和多模态输入、输出技术以及人机交互技术等相结合，构成人-机对话系统。</li><li>信息过滤：通过计算机系统自动识别和过滤满足特定条件的文档信息。</li><li>信息抽取：指从文本中抽取出特定的事件(event)或事实信息，又称事件抽取。信息抽取与信息检索不同，信息抽取直接从自然语言文本中抽取信息框架，一般是用户感兴趣的事实信息，而信息检索主要是从海量文档集合中找到用户需求(一般通过<strong>关键词</strong>表达)相关的文档列表。</li><li>文本挖掘：又称数据挖掘，指从文本中获取高质量信息的过程。</li><li>舆情分析：舆情是指在一定的社会空间内，围绕中介性社会事件的发生、发展和变化，民众对社会管理者产生和持有的社会政治态度。它是较多群众关于社会中各种现象、问题所表达的信念、态度、意见和情绪等等表现的总和。</li><li>隐喻计算：”隐喻”就是用乙事物或其某些特征来描述甲事物的语言现象，简单地讲，<strong>隐喻计算就是研究自然语言语句或篇章中隐喻修辞的理解方法</strong>。</li><li>文字编辑和自动校对：对文字拼写、用词，甚至语法、文档格式等进行自动检查、校对和编排。</li><li>作文自动评分：对作文质量和写作水平进行自动评价和打分。</li><li>光读字符识别：通过计算机系统对印刷体或手写体等文字进行自动识别，将其转换成计算机可以处理的电子文本。</li><li>语音识别：将输入计算机的语音信号识别转换成书面语表示。</li><li>文语转换：将书面文本自动转换成对应的语音表征。</li><li>说话人识别：对一说话人的言语样本做声学分析，依此判断说话人身份。</li></ul><h3 id="2-2-NLP涉及的几个层次"><a href="#2-2-NLP涉及的几个层次" class="headerlink" title="2.2 NLP涉及的几个层次"></a>2.2 NLP涉及的几个层次</h3><ul><li>形态学(morphology)：又称”词汇形态学”或”词法”，是语言学的一个分支，研究词的内部结构，包括屈折变化和构词法两部分。词具有语音特征、句法特征和语义特征，是每个语言学家都要关注的一门学科。</li><li>语法学(syntax)：研究句子结构成分之间的相互关系和组成句子序列的规则，其关注的中心是：<strong>为什么一句话可以这么说，也可以这么说？</strong></li><li>语义学(semantics)：是一门研究意义，特别是语言意义的学科。它所关注的重点是：<strong>这个语言单位到底说了什么？</strong></li><li>语用学(pragmatics)：是现代语言学用来指从使用者的角度研究语言，特别是使用者所做的选择、他们在社会互动中所受的制约、他们的语言使用对信递活动中其他参与者的影响。其关注的重点是：<strong>为什么在特定的上下文中要说这句话？</strong></li></ul><h3 id="2-3-NLP面临的困难"><a href="#2-3-NLP面临的困难" class="headerlink" title="2.3 NLP面临的困难"></a>2.3 NLP面临的困难</h3><p>&emsp;&emsp;进一步归纳总结，NLP领域实现上述应用目标最终需要解决的关键问题就是<strong>歧义消解(disambiguation)问题</strong>和<strong>未知语言现象的处理问题</strong>。</p><h2 id="3-NLP的基本方法及其发展"><a href="#3-NLP的基本方法及其发展" class="headerlink" title="3. NLP的基本方法及其发展"></a>3. NLP的基本方法及其发展</h2><h3 id="3-1-NLP的基本方法"><a href="#3-1-NLP的基本方法" class="headerlink" title="3.1 NLP的基本方法"></a>3.1 NLP的基本方法</h3><p>&emsp;&emsp;一般认为，NLP中存在着两种不同的研究方法：<strong>理性主义(rationalist)方法和经验主义(empiricist)方法</strong>。</p><ul><li>理性主义方法认为人的很大一部分语言知识是与生俱来的，由遗传决定的。试图通过假定人的语言能力是与生俱来的、固有的一种本能来回避这些困难的问题，在具体的自然语言问题研究中，主张建立符号处理系统，由人工整理和编写初始的语言知识表示体系(通常为<strong>规则</strong>)，构造相应的推理程序，系统根据规则和程序，将自然语言理解为符号结构——该结构的意义可以从结构中的符号的意义推导出来。</li><li>经验主义方法也是假定人脑所具有的一些认知能力开始的。因此，从某种意义上讲两种方法并不是绝对对立的。但是其假定人脑一开始具有处理联想、模式识别和通用化处理的能力。在系统实现上，经验主义方法主张通过建立特定的数学模型来学习复杂的、广泛的语言结构，然后利用统计学、模式识别和机器学习等方法来训练模型的参数，以扩大语言使用的规模。我们又称其为统计自然语言处理。</li></ul><h2 id="4-NLP的研究现状"><a href="#4-NLP的研究现状" class="headerlink" title="4. NLP的研究现状"></a>4. NLP的研究现状</h2><ul><li>已开发完成一批颇具影响的语言资源库，部分技术已达到或基本达到实用化程度，并在实际应用中发挥着巨大作用。</li><li>许多新的研究方向不断出现。</li><li>许多理论问题尚未得到根本性的解决。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 读书笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP基础 </tag>
            
            <tag> 《统计自然语言处理》 </tag>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>关于</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="关于博客"><a href="#关于博客" class="headerlink" title="关于博客"></a>关于博客</h2><hr><ul><li>主要记录自己日常的学习笔记、论文笔记等内容</li><li>目前博客很简陋，后期看情况折腾吧</li><li>此博客由<a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>强力驱动，并部署在<a href="https://github.com/" target="_blank" rel="noopener">Gayhub上</a>(逃)</li></ul><h2 id="博客日志"><a href="#博客日志" class="headerlink" title="博客日志"></a>博客日志</h2><hr><ul><li>2020-01-18 建站</li><li>2020-01-19 主题优化，添加搜索、评论等功能</li><li>2020-01-20 添加相册功能</li></ul><h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><hr><ul><li>95后，目前大四，已确定读研</li><li>本科就读于东北大学(沈阳)计算机科学与技术专业</li><li>硕士就读于中科院计算所(网络数据科学与技术实验室)，方向NLP</li><li>邮箱：<a href="mailto:zhangyqCS@163.com">zhangyqCS@163.com</a></li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>相册</title>
      <link href="/photos/index.html"/>
      <url>/photos/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="❤NEU"><a href="#❤NEU" class="headerlink" title="❤NEU"></a>❤NEU</h2><!-- <div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column"  style="width: 33.333333333333336%;"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfhre7wj31z418g4qs.jpg" alt="樱"></div><div class="group-picture-column"  style="width: 33.333333333333336%;"><img src="http://tva2.sinaimg.cn/large/006Ahf5Fly1gb1yfiviekj31hc0xcn82.jpg" alt="晚霞"></div><div class="group-picture-column"  style="width: 33.333333333333336%;"><img src="http://tva4.sinaimg.cn/large/006Ahf5Fly1gb1yfjd0efj31qu0zkdoz.jpg" alt="傍晚"></div></div><div class="group-picture-row"><div class="group-picture-column"  style="width: 100%;"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfm15e1j31z418g7wi.jpg" alt="海贼王"></div></div></div></div><p> –&gt;</p><!-- <center>    <figure>        <img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfhre7wj31z418g4qs.jpg" width="300px" height="200px" />图1        <img src="http://tva2.sinaimg.cn/large/006Ahf5Fly1gb1yfiviekj31hc0xcn82.jpg" width="300px" height="300px" />图2    </figure></center> --><table>    <tr>        <td><img src="http://tva3.sinaimg.cn/large/006Ahf5Fly1gb348qljo4j31900s0aff.jpg" ></td>        <td><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb34cl6363j33401yghdv.jpg" ></td>        <td><img src="http://tva1.sinaimg.cn/large/006Ahf5Fly1gb348qpfwgj31990s0wls.jpg" ></td>    </tr><!--     <tr>        <td colspan="2"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb34cl6363j33401yghdv.jpg" ></td>    </tr>    <tr>        <td colspan="2"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfm15e1j31z418g7wi.jpg" ></td>    </tr> --></table>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
