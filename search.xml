<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>社会网络分析-基础知识</title>
      <link href="/2020/03/21/%E7%A4%BE%E4%BC%9A%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
      <url>/2020/03/21/%E7%A4%BE%E4%BC%9A%E7%BD%91%E7%BB%9C%E5%88%86%E6%9E%90-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp;<strong>社会网络分析</strong>是研究一组行为者的关系的研究方法，一组行为者可以是人、社区、群体等，他们的关系模式反映出的现象或数据是网络分析的焦点。社会网络分析以数据挖掘为基础，采用可视化的图表以及社会网络结构的形式表示，运用这种研究方法，可以建立社会关系模型、发现社群内部行动者之间的各种社会关系。</p><hr><h3 id="1-网络中关键节点的识别"><a href="#1-网络中关键节点的识别" class="headerlink" title="1. 网络中关键节点的识别"></a>1. 网络中关键节点的识别</h3><h4 id="1-1-衡量方式"><a href="#1-1-衡量方式" class="headerlink" title="1.1 衡量方式"></a>1.1 衡量方式</h4><ul><li><p>度：最简单也是最直观的衡量方式，节点的度数越大，说明与该节点相连的节点就越多，该节点越重要，例如微博大V。<strong>优点</strong>是计算简单，<strong>缺点</strong>是缺乏全局考虑，仅考虑了1度关联的结点数，没有考虑关联结点的重要性，例如某大V购买了很多僵尸粉，但这个“大V”对正常用户可能影响很小。</p></li><li><p>介数：节点的介数被定义为<strong>网络中所有的最短路径经过该节点的数目</strong>，介数越高，说明网络中任意两个节点的关系与该节点关系越大，即该节点影响力越大。<strong>优点</strong>是相比度，介数考虑节点在整个网络中的重要程度，<strong>缺点</strong>是计算时间复杂度高，在节点较多的复杂网络中不适用。</p></li><li><p>核度：节点的核度被定义为，对网络从外层一层层地剥离直到没有节点，该节点处于被剥离的位置。节点的核度越大，越是最后被剥离，说明它越处于网络的中心位置，也就越重要。值得注意的是，<strong>并不是度越大的节点，其核度越大</strong>。<strong>优点</strong>是相比度的局限性，核度考虑了节点在全局的重要程度，并且没有计算介数复杂度这么高。缺点是划分粒度太粗，导致很多感觉上并不属于同一层级的节点被同时剥离，即每一次剥离的节点很多。</p></li></ul><h4 id="1-2-经典算法-PageRank"><a href="#1-2-经典算法-PageRank" class="headerlink" title="1.2 经典算法-PageRank"></a>1.2 经典算法-PageRank</h4><p>&emsp;&emsp;简单来说，一个网页的影响力=所有入链集合的页面的加权影响力之和：$PR(u) = \sum_{v \in B_u} \frac{PR(v)}{L(v)}$。其中$u$为待评估的页面，$B_u$为页面$u$的入链集合。针对入链集合中的任意页面$v$，它能带给$u$的影响力是其自身的影响力除以$v$页面的出链数量，即页面$v$把自身的影响力$PR(v)$平均分配给它的出链。出链会给被链接的页面赋予影响力，当我们统计一个网页出链的数量，也就能统计该页面的跳转概率。</p><div align=center><img src="http://tva4.sinaimg.cn/large/006Ahf5Fly1gd2mmco03fj309n06fmz1.jpg"></div><p>&emsp;&emsp;这个例子中，A有三个出链分别连接到B、C和D，当用户访问A页面时，就有机会跳转到B、C、D页面的可能，其概率均为$\frac{1}{3}$。同理，我们可求得A、B、C、D四个页面的转移矩阵：$$M = \begin{bmatrix}<br>{0} &amp; { 1/2 } &amp; {1} &amp; {0}\<br>{ 1/3 } &amp; {0} &amp; {0} &amp; { 1/2 }\<br>{ 1/3 } &amp; {0} &amp; {0} &amp; { 1/2 }\<br>{ 1/3 } &amp; { 1/2 } &amp; {0} &amp; {0}\<br>\end{bmatrix}$$</p><p>我们假设初始时，页面A、B、C、D影响力是相同的，即：$$ w_0 = \begin{bmatrix}<br>{ 1/4 }\<br>{ 1/4 }\<br>{ 1/4 }\<br>{ 1/4 }\<br>\end{bmatrix}$$<br>当第一次转移后，各页面的影响力$w_1$变为：$$ w_1 = \begin{bmatrix}<br>{0} &amp; { 1/2 } &amp; {1} &amp; {0}\<br>{ 1/3 } &amp; {0} &amp; {0} &amp; { 1/2 }\<br>{ 1/3 } &amp; {0} &amp; {0} &amp; { 1/2 }\<br>{ 1/3 } &amp; { 1/2 } &amp; {0} &amp; {0}\<br>\end{bmatrix} \begin{bmatrix}<br>{ 1/4 }\<br>{ 1/4 }\<br>{ 1/4 }\<br>{ 1/4 }\<br>\end{bmatrix} = \begin{bmatrix}<br>{ 9/24 }\<br>{ 5/24 }\<br>{ 5/24 }\<br>{ 5/24 }\<br>\end{bmatrix}$$</p><p>然后，我们再用转移矩阵乘以$w_1$得到$w_2$，一直迭代，直至$w_n$影响力不再发生变化，所求的$w_n$对于各个页面最终的影响力。至此我们模拟了一个简化的PageRank的计算过程，实际情况中可能面临如下两个问题：</p><ul><li>等级泄露(Rank Leak)：如果一个页面没有出链，就像一个黑洞，吸收了其他页面的影响力而不释放，最终会导致其他页面影响力为0。</li><li>等级沉没(Rank Sink)：如果一个页面只有出链，没有入链，计算的过程迭代下来，会导致这个页面的$PR$值为0。</li></ul><p>&emsp;&emsp;为了解决简化模型中等级泄露和等级沉没的问题，原作者提出了PageRank的随机浏览模型，该模型假设：<strong>用户并不都是按照跳转链接的方式来上网，还有一种可能就是不论当前处于哪个页面，都有一定概率访问其他页面，比如用户直接输入网址访问某个页面，虽然这种概率比链接访问的概率小</strong>。所以该模型定义了阻尼因子$d$代表了用户按链接跳转访问页面的，通常可以取固定值$d=0.85$，则影响力计算公式可改为：$PR(u) = \frac{1-d}{N} + d \sum_{v \in B_u} \frac{PR(v)}{L(v)}$。其中$N$为总页面数。在一定程度上可以缓解等级沉没和等级泄露的问题。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://www.jianshu.com/p/37964b2f538f" target="_blank" rel="noopener">网络中关键节点的识别-简书</a></li><li><a href="https://www.cnblogs.com/jpcflyer/p/11180263.html" target="_blank" rel="noopener">机器学习经典算法之PageRank-个人博客</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 社会网络分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 社会网络分析 </tag>
            
            <tag> 理论知识 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯决策理论</title>
      <link href="/2020/02/07/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA/"/>
      <url>/2020/02/07/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp; <strong>贝叶斯决策理论(Bayesian decision theory)</strong> 是统计方法处理模式分类问题的基本理论之一。在之前的学习过程中接触过该理论，但一直没理解其思想。本文试图探讨以下问题：</p><ul><li>贝叶斯决策理论是什么？有什么用处？</li><li>贝叶斯决策理论中”先验概率”，”后验概率”，”贝叶斯公式”是什么？</li><li>如果说贝叶斯决策理论是最优的，为什么还需要其他算法？</li></ul><hr><p>&emsp;&emsp;机器学习(Machine Learning)可以理解为<strong>通过观测数据$(X)$推测结果$(y)$，同时学习推断规则$(c)$，使得推断结果$\hat y = c(X)$与真实值$y$的误差尽可能小</strong>。举个例子，我们现在有一批西瓜(为简单起见，现假设仅通过根蒂就能判断西瓜的好坏)，并观察到了它们的根蒂形状$(X)$，就可以通过推断规则$(c)$来估计西瓜是好瓜还是坏瓜$(y)$。全文我们都会举西瓜的例子，我们能观察到的西瓜根蒂$X$(蜷缩、硬挺、稍蜷)，西瓜的好坏$y$：好瓜$(y=1)$、坏瓜$(y=0)$。</p><h3 id="1-在没有其他信息时，我们如何推断？"><a href="#1-在没有其他信息时，我们如何推断？" class="headerlink" title="1. 在没有其他信息时，我们如何推断？"></a>1. 在没有其他信息时，我们如何推断？</h3><ul><li>假设我们第一次接触判断西瓜好坏的问题，什么都不知道。此时只能预测好瓜和坏瓜的概率都是0.5。</li><li>假设我们虽然第一次接触判断西瓜好坏的问题，但神告诉我们这种西瓜是好瓜的概率$P(1) = 0.7$，是坏瓜的概率$P(0) = 0.3$。显然，此时我们倾向于预测西瓜是好瓜，且预测错误的概率是$1-P(1) = P(0) = 0.3$。</li></ul><p>&emsp;&emsp;此处的$P(1)=0.7$和$P(0)=0.3$就叫做<strong>先验概率(prior probability)</strong>，指的是在观测前我们就已知结果的概率分布$P(y)$。</p><h3 id="2-当有了更多信息时，该如何利用它们？"><a href="#2-当有了更多信息时，该如何利用它们？" class="headerlink" title="2. 当有了更多信息时，该如何利用它们？"></a>2. 当有了更多信息时，该如何利用它们？</h3><p>&emsp;&emsp;显然，前文的推测方式是很不准确的，因为没有考虑到西瓜的属性。而现实情况是我们往往可以观察到西瓜的一些属性，并非完全一无所知。因此，我们尝试回答：在观察到西瓜的根蒂形状后，它是好瓜的概率是多少？对应的数学表达式就是$P(y=1 \mid X)$。相应的，是坏瓜的数学表达式为$P(y=0 \mid X)$。</p><p>&emsp;&emsp;此处的$P(y=1 \mid X)$和$P(y=0 \mid X)$就叫做<strong>后验概率(posterior probability)</strong>，指的是在观测到$X$后我们对结果$y$的估计。可以简单地理解为，<strong>后验概率是对先验概率的一种”更加细致准确的刻画”，因为此时我们观测到了$X$，有了额外的信息，所以后验概率比先验概率更有意义</strong>。大部分机器学习模型尝试得到的就是后验概率。</p><h3 id="3-如何求得后验概率？"><a href="#3-如何求得后验概率？" class="headerlink" title="3. 如何求得后验概率？"></a>3. 如何求得后验概率？</h3><p>&emsp;&emsp;我们无法直接获得后验概率，但<strong>可以通过贝叶斯公式计算得到</strong>。根据条件概率易知：<br>$$P(A \mid B) = \frac{P(AB)}{P(B)} \Rightarrow P(AB) = P(A \mid B) \cdot P(B) \tag{1}$$ $$P(B \mid A) = \frac{P(BA)}{P(A)} \Rightarrow P(BA) = P(B \mid A) \cdot P(A) \tag{2}$$</p><p>因为$P(AB) = P(BA)$，所以$P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A)$，进而推出了大名鼎鼎的<strong>贝叶斯公式</strong>：$$P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)} \tag{3}$$ 将其带入到上文中的后验概率中，可求得：$$P(y=1 \mid X) = \frac{P(X \mid y=1) \cdot P(y=1)}{P(X)} \tag{4}$$ $$P(y=0 \mid X) = \frac{P(X \mid y=0)\cdot P(y=0)}{P(X)} \tag{5}$$ 我们发现，在通过贝叶斯公式变形后，后验概率变得可以计算了。<strong>贝叶斯决策的预测值是选取后验概率最大的结果</strong>。</p><h3 id="4-为什么贝叶斯决策理论是”最优的”？"><a href="#4-为什么贝叶斯决策理论是”最优的”？" class="headerlink" title="4. 为什么贝叶斯决策理论是”最优的”？"></a>4. 为什么贝叶斯决策理论是”最优的”？</h3><p>&emsp;&emsp;因为$P(y=0 \mid X) = 1- P(y=1 \mid X)$，<strong>只要观察到$X$，并选择后验概率最大的结果，就可以最小化预测的错误概率。这个结论对所有的观测值$X$均成立，所以才叫做”最优”</strong>。</p><h3 id="5-贝叶斯决策理论有什么用？"><a href="#5-贝叶斯决策理论有什么用？" class="headerlink" title="5. 贝叶斯决策理论有什么用？"></a>5. 贝叶斯决策理论有什么用？</h3><p>&emsp;&emsp;我们定义最小误分概率(最优情况下):$$R^* = \sum_X P(X) \cdot min(P(y=0 \mid X), P(y=1 \mid X)) \tag{6}$$ 易知$\sum_X P(X)=1$且$0 \leq P(X_i) \leq 1$。在二分类的情况下$0 \leq min(P(y=0 \mid X), P(y=1 \mid X)) \leq 0.5$，因此最小误分概率$0 \leq R^* \leq 0.5$。<br>&emsp;&emsp;举个例子，假设在所有的观测$X$下，$min(P(y=0 \mid X), P(y=1 \mid X)) \leq 0.2$成立，那么我们可以知道这个决策的最大误分概率就是$0.2$，即<strong>不存在任何其他决策规则会得到低于$0.2$的误分概率</strong>。所以贝叶斯决策理论可以用来<strong>估算理论上的计算边界(Bound)，尤其是误差的上界</strong>。</p><h3 id="6-如果说贝叶斯决策理论是最优的，为什么还需要其他算法？"><a href="#6-如果说贝叶斯决策理论是最优的，为什么还需要其他算法？" class="headerlink" title="6. 如果说贝叶斯决策理论是最优的，为什么还需要其他算法？"></a>6. 如果说贝叶斯决策理论是最优的，为什么还需要其他算法？</h3><p>&emsp;&emsp;既然贝叶斯决策理论是最优的，为什么不能盖过深度学习的锋芒呢？其根本原因在于贝叶斯决策中假设比较强，实际操作起来并不容易：</p><ul><li>无法获得真实的先验概率等其他计算要素，需要通过统计学的方法进行估计，存在一定的误差。</li><li>计算所需要的概率往往是不可能的，会面临维度灾难(the curse of dimensionality)。在条件概率$P(y=0 \mid X)$和$P(y=1 \mid X)$中，要考虑观察的特征不止一个，可能还有其他特征。而且实际计算时往往面临很多数值稳定性问题，所以朴素贝叶斯才会介绍平滑(smoothing)来解决一些特例。</li></ul><p>&emsp;&emsp;综上所述，<strong>直接利用贝叶斯决策理论进行后仰概率估算是不大实用的，但对于理论上计算边界，尤其是误差的上界比较有意义</strong>。</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://www.zhihu.com/question/27670909/answer/378129817" target="_blank" rel="noopener">如何简单理解贝叶斯决策理论?</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> ML理论知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 理论知识 </tag>
            
            <tag> Machine Learning基础 </tag>
            
            <tag> 贝叶斯决策理论 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>关于</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="关于博客"><a href="#关于博客" class="headerlink" title="关于博客"></a>关于博客</h2><hr><ul><li>主要记录自己日常的学习笔记、论文笔记等内容</li><li>目前博客很简陋，后期看情况折腾吧</li><li>此博客由<a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>强力驱动，并部署在<a href="https://github.com/" target="_blank" rel="noopener">Gayhub上</a>(逃)</li></ul><h2 id="博客日志"><a href="#博客日志" class="headerlink" title="博客日志"></a>博客日志</h2><hr><ul><li>2020-01-18 建站</li><li>2020-01-19 主题优化，添加搜索、评论等功能</li><li>2020-01-20 添加相册功能</li></ul><h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><hr><ul><li>95后，目前大四，已确定读研</li><li>本科就读于东北大学(沈阳)计算机科学与技术专业</li><li>硕士就读于中科院计算所(网络数据科学与技术实验室)，方向NLP</li><li>邮箱：<a href="mailto:zhangyqCS@163.com">zhangyqCS@163.com</a></li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>相册</title>
      <link href="/photos/index.html"/>
      <url>/photos/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="❤NEU"><a href="#❤NEU" class="headerlink" title="❤NEU"></a>❤NEU</h2><!-- <div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column"  style="width: 33.333333333333336%;"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfhre7wj31z418g4qs.jpg" alt="樱"></div><div class="group-picture-column"  style="width: 33.333333333333336%;"><img src="http://tva2.sinaimg.cn/large/006Ahf5Fly1gb1yfiviekj31hc0xcn82.jpg" alt="晚霞"></div><div class="group-picture-column"  style="width: 33.333333333333336%;"><img src="http://tva4.sinaimg.cn/large/006Ahf5Fly1gb1yfjd0efj31qu0zkdoz.jpg" alt="傍晚"></div></div><div class="group-picture-row"><div class="group-picture-column"  style="width: 100%;"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfm15e1j31z418g7wi.jpg" alt="海贼王"></div></div></div></div><p> –&gt;</p><!-- <center>    <figure>        <img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfhre7wj31z418g4qs.jpg" width="300px" height="200px" />图1        <img src="http://tva2.sinaimg.cn/large/006Ahf5Fly1gb1yfiviekj31hc0xcn82.jpg" width="300px" height="300px" />图2    </figure></center> --><table>    <tr>        <td><img src="http://tva3.sinaimg.cn/large/006Ahf5Fly1gb348qljo4j31900s0aff.jpg" ></td>        <td><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb34cl6363j33401yghdv.jpg" ></td>        <td><img src="http://tva1.sinaimg.cn/large/006Ahf5Fly1gb348qpfwgj31990s0wls.jpg" ></td>    </tr><!--     <tr>        <td colspan="2"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb34cl6363j33401yghdv.jpg" ></td>    </tr>    <tr>        <td colspan="2"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfm15e1j31z418g7wi.jpg" ></td>    </tr> --></table>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
