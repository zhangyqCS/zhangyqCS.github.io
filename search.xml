<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>贝叶斯决策理论</title>
      <link href="/2020/12/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM/"/>
      <url>/2020/12/30/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp; 记得很早就接触过 SVM 了，但总感觉一直没有完全把 SVM 整明白，之前也没有做过详细的公式推导。趁着最近复习 PRML 的期末考试，自己把 SVM 的原理、公式推导重新学习了一遍，所以写个博客来总结一下。</p><hr><h3 id="1-直观理解"><a href="#1-直观理解" class="headerlink" title="1. 直观理解"></a>1. 直观理解</h3><p>&emsp;&emsp; 简单来讲，SVM 就是一种二类分类模型，其核心思想为通过<strong>最大化分类间隔</strong>来获得最鲁棒的分类效果。</p><div align=center><img src="https://tvax4.sinaimg.cn/large/006Ahf5Fly1gm6aft8pl6j31ev0pu75z.jpg"></div><div align=center>图 1. 多种分类平面</div><p>&emsp;&emsp; 例如图1中，三个分类面都可以将样本正确分类，但哪一个分类面的分类效果更好呢？</p><p>&emsp;&emsp; 直觉告诉我们显然中间绿色的分类面效果更好。因为对于其他分类面，距离分类面最近的样本与分类面的间隔较小，如果样本数据存在噪声的话(显然噪声是一定存在的)，很有可能被错误分类，即<strong>其他分类面对数据噪声敏感，泛化能力弱</strong>。</p><p>&emsp;&emsp; SVM 通过选择以最大化间隔将两类样本分类的超平面作为最佳分类面。以上介绍的 SVM 只能处理线性可分的样本数据，为了解决更加复杂的问题，SVM 有一由简到繁的模型：</p><ul><li><p>线性可分 SVM ：当样本数据<strong>线性可分</strong>时，通过硬间隔(Hard Margin)最大化来学习线性分类器，如上图绿色分类面。</p></li><li><p>线性 SVM ：当样本数据<strong>接近线性可分</strong>时，通过软间隔(Soft Margin)最大化来学习线性分类器。</p></li><li><p>非线性 SVM ：当样本数据<strong>线性不可分</strong>时，通过使用核函数(Kernel Function)和软间隔最大化来学习非线性分类器。</p></li></ul><h3 id="2-线性可分-SVM-——硬间隔"><a href="#2-线性可分-SVM-——硬间隔" class="headerlink" title="2. 线性可分 SVM ——硬间隔"></a>2. 线性可分 SVM ——硬间隔</h3><p>&emsp;&emsp; 考虑如下线性可分的样本数据：$(x_1, y_1), (x_2, y_2), …, (x_N, y_N)$。其中 $x_i$为 $d$ 维列向量，即 $x_i \in R^d$，$y_i \in {+1, -1}$ 为标量，$y_i = +1$ 时表示 $x_i$ 属于正类别，$y_i = -1$ 时表示 $x_i$ 属于负类别。</p><h4 id="2-1-分类间距及支持向量"><a href="#2-1-分类间距及支持向量" class="headerlink" title="2.1 分类间距及支持向量"></a>2.1 分类间距及支持向量</h4><div align=center><img src="https://tva2.sinaimg.cn/large/006Ahf5Fly1gm6zgpqthbj31f90ps403.jpg"></div><div align=center>图 2. 几何距离</div><p>如上图所示，样本点 $x_i$ 到分类平面的几何距离可以表示为：$$\frac{\lvert w^T \cdot x_i +b \rvert}{\lVert w \rVert} \tag{1}$$</p><p>与分类平面距离最近的样本点称为<strong>支持向量</strong>，进而构成<strong>支持平面</strong>：</p><div align=center><img src="https://tvax1.sinaimg.cn/large/006Ahf5Fly1gm6zld6lfbj31fz0q0wg7.jpg"></div><div align=center>图 3. 支持向量与支持平面</div><p>分类器的分类间距 $\rho$ 指的是支持平面之间的距离：</p><div align=center><img src="https://tva1.sinaimg.cn/large/006Ahf5Fly1gm6zqn5jk7j31gx0qd407.jpg"></div><div align=center>图 4. 分类间距</div><h4 id="2-2-间隔最大化"><a href="#2-2-间隔最大化" class="headerlink" title="2.2 间隔最大化"></a>2.2 间隔最大化</h4><p>对于上述线性可分训练集中任意样本 $(x_i, y_i)$ 均满足：</p><ul><li><p>如果 $y_i = -1$，则 $w^T \cdot x_i +b \leq - \frac{\rho}{2}$</p></li><li><p>如果 $y_i = +1$，则 $w^T \cdot x_i +b \geq + \frac{\rho}{2}$</p></li></ul><p>即对于任意样本，函数距离满足以下不等式：$$y_i \cdot (w^T \cdot x_i +b) \geq \frac{\rho}{2} \tag{2}$$</p><p>由于参数除以一个常数不影响结果，可以将 公式(2) 进一步化简为：$$y_i \cdot (w^T \cdot x_i +b) \geq 1 \tag{3}$$</p><p>根据支持向量的定义，可以对上述不等式取等：$$y_s \cdot (w^T \cdot x_s +b) = 1 \tag{4}$$</p><p>其中 $x_s$ 表示支持向量，$y_s$ 表示对应的类别。</p><p>由于 $\lvert y_s \rvert = 1$，必然有 $\lvert w^T \cdot x_s +b \rvert = 1$，因此支持向量到分类平面的几何距离为：$$\frac{\rho}{2} = \frac{\lvert w^T \cdot x_s +b \rvert}{\lVert w \rVert} = \frac{1}{\lVert w \rVert} \tag{5}$$</p><p>最大化分类间隔 $\rho$ 等价于以下问题：$$max_{w,b} \ \rho = max_{w,b} \ {\rho}^2 = min_{w,b} \ \frac{1}{2} \cdot {\lVert w \rVert}^2 \tag{6}$$</p><p>其中的 $\frac{1}{2}$ 没有实际意义，仅仅为了求导方便，这样就消除了非参数变量 $\rho$。</p><h4 id="2-3-约束条件"><a href="#2-3-约束条件" class="headerlink" title="2.3 约束条件"></a>2.3 约束条件</h4><p>&emsp;&emsp; 支持向量机的目标是最大化分类间隔(即几何距离)，但一个重要前提是必须保证分类正确，即距离函数应满足 公式(3) 。可以将 $x_i$ 与 $w$ 写成增广矩阵的形式来消除偏置项 $b$：$$x_i = [x_i^T , 1]^T \ \ \ \ w= [w^T, b]^T \tag{7}$$</p><p>&emsp;&emsp; 综上所述，支持向量机的优化目标是在保证分类正确的前提下最大化分类间隔，其形式化的表述为：$$\hat{w} = argmin_w {\frac{1}{2} \lVert w \rVert}$$$$s.t. \ \ \ \ y_i w^T x_i \geq 1, \forall i=1, 2, …, N \tag{8}$$</p><h4 id="2-4-参数求解"><a href="#2-4-参数求解" class="headerlink" title="2.4 参数求解"></a>2.4 参数求解</h4><p>&emsp;&emsp; 受限条件下的优化问题通常使用拉格朗日乘子法(Lagrange Multiplier)来求解。需要注意的是此处的约束条件是不等式，因此优化目标可以定义为：$$L(w, \alpha) = \frac{1}{2} \lVert w \rVert^2 - \sum_{i=1}^N \alpha_i (y_i w^T x_i -1) $$$$s.t. \ \ \ \ \alpha_i \geq 0, \forall i=1, 2, …, N \tag{9}$$</p><p>由于 $y_i w^T x_i \geq 1$ 且 $\alpha_i \geq 0$，则有：$$- \sum_{i=1}^N \alpha_i (y_i w^T x_i -1) \leq 0 \tag{10}$$</p><p>因此优化问题等价于求解：$$(\hat{w}, \hat{\alpha}) = argmin_w \ max_{\alpha} \ L(w,\alpha) \tag{11}$$</p><p>&emsp;&emsp; SVM 采用了一种巧妙的方法，将公式 (11) 对应的<strong>原始问题</strong> $min-max$ 求解转换为<strong>对偶问题</strong> $max-min$ 的求解：$$(\hat{w}, \hat{\alpha}) = argmax_{\alpha} \ min_{w} \ L(w,\alpha) \tag{12}$$</p><blockquote><p>原始问题与对偶问题同时取得最优解需要满足 <strong>KKT 条件</strong>。<br>考虑一个通用的约束优化问题：$min \ f(x) s.t. \ \ g(x) \leq 0$，KKT条件主要包括以下四个子条件：</p><ol><li>定常方程式：$\frac{\partial f}{\partial x} + \lambda \frac{\partial g}{\partial x} =0$</li><li>原始可行性： $g(x) \leq 0$</li><li>对偶可行性： $\lambda \geq 0$</li><li>互补松弛性：$\lambda g(x) = 0$</li></ol><p>公式 (9) 对应的子条件为：</p><ol><li>定常方程式：$\frac{\partial L}{\partial w} = 0$</li><li>原始可行性：$y_i w^T x_i \geq 1, \forall i=1,2, …, N$</li><li>对偶可行性：$\alpha_i \geq 0, \forall i=1,2, …, N$</li><li>互补松弛性：$\alpha_i (y_i w^T x_i -1) =0, \forall i=1,2,…, N$</li></ol><p>其中第4点的含义是，<strong>支持向量的样本点满足 $y_i w^T x_i =1$，而且其对应的$\alpha_i &gt; 0$，而非支持向量的样本点对应的$\alpha_i = 0$</strong>。 </p></blockquote><p>&emsp;&emsp; 针对公式 (12) 的对偶问题求解目标，首先考虑层内的计算最小值问题：$min_w \ L(w,\alpha)$。计算目标函数 $L(w, \alpha)$关于$w$的偏导，令其为0可以得到：$$\frac{\partial L(w, \alpha)}{\partial w} = w - \sum_{i=1}^N \alpha_i y_i x_i \ \ \ \ \ \ w= \sum_{i=1}^N \alpha_i y_i x_i \tag{13}$$</p><p>将公式 (13) 代入公式 (9)，得到：$$L(\hat{w}, \alpha) = \frac{1}{2} \lVert \hat{w} \rVert^2 - \sum_{i=1}^N \alpha_i (y_i \hat{w}^T x_i -1)$$$$= \frac{1}{2} \sum_{i=1}^N \alpha_i y_i x_i^T \sum_{j=1}^N \alpha_j y_j x_j - \sum_{i=1}^N \alpha_i y_i x_i^T \sum_{j=1}^N \alpha_j y_j x_j + \sum_{i=1}^N \alpha_i$$$$= \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \alpha_i y_i x_i^T \sum_{j=1}^N \alpha_j y_j x_j = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i,j=1}^N y_i y_j \alpha_i \alpha_j x_i^T x_j \tag{14}$$</p><p>可以利用 SMO 算法来求解 $\hat{\alpha}$：$$\hat{\alpha} = argmax_{\alpha} { \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i,j=1}^N y_i y_j \alpha_i \alpha_j x_i^T x_j } \tag{15}$$</p><h3 id="3-线性-SVM-——软间隔"><a href="#3-线性-SVM-——软间隔" class="headerlink" title="3. 线性 SVM ——软间隔"></a>3. 线性 SVM ——软间隔</h3><h4 id="3-1-最大化软间隔及参数求解"><a href="#3-1-最大化软间隔及参数求解" class="headerlink" title="3.1 最大化软间隔及参数求解"></a>3.1 最大化软间隔及参数求解</h4><p>&emsp;&emsp; 当样本数据接近线性可分时，可以引入松弛变量来容忍部分不可分的样本数据：$$y_i w^T x_i \geq 1- \xi \tag{16}$$</p><div align=center><img src="https://tva1.sinaimg.cn/large/006Ahf5Fly1gm73dykqq6j31d30nf3zi.jpg"></div><div align=center>图 5. 松弛变量</div><p>引入松弛变量后，支持向量机的优化目标扩展为：$$min_{w, \xi} \ \frac{1}{2} \lVert w \rVert^2 + C \sum_{i=1}^N \xi_i$$$$s.t. \ \ y_i w^T x_i \geq 1- \xi_i,\ \forall i=1, 2, …, N$$$$\xi_i \geq 0, \ \forall i=1,2, …, N \tag{17}$$</p><p>&emsp;&emsp; 其中远离群体的样本点称为<strong>离群点</strong>。每个离群点$x_i$都有一个$\xi_i &gt; 0$，表示其远离的程度，而非离群点对应 $\xi_i = 0$。$C$ 是一个超参数，其取值越大，表示越重视离群点，不希望舍弃这些样本点。因此新的优化目标是<strong>最大化分类间隔，同时最小化离群距离</strong>。</p><p>使用拉格朗日乘子法，可以得到目标函数如下：$$L(w, \xi, \alpha, \beta) = \frac{1}{2} \lVert w \rVert^2 + C \sum_{i=1}^N \xi_i - \sum_{i=1}^N \alpha_i (y_i w^T x_i -1 + \xi_i) - \sum_{i=1}^N \beta_i \xi_i \tag{18}$$</p><p>原始问题可以表述为：$$min_{w,\xi} \ max_{\alpha, \beta} \ L(w, \xi, \alpha, \beta) \tag{19}$$</p><p>对偶问题可以表述为：$$max_{\alpha, \beta} \ min_{w,\xi} \ L(w, \xi, \alpha, \beta) \tag{20}$$</p><p>接下来我们可以求解对偶问题获得原始问题的解。首先计算目标函数关于$w, \xi$的极值：$$\frac{\partial L}{\partial w} = 0 \ \ \Longrightarrow \ \ \hat{w} = \sum_{i=1}^N \alpha_i y_i x_i$$$$\frac{\partial L}{\partial \xi} = 0 \ \ \Longrightarrow \ \ C = \alpha_i + \beta_i \ \ \forall i=1, 2, …, N \tag{21}$$</p><p>代入目标函数，可以得到以下优化公式：$$\hat{\alpha} = argmax_{0 \leq \alpha \leq C} { \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i,j=1}^N y_i y_j \alpha_i \alpha_j x_i^T x_j } \tag{22}$$</p><p>仍然可以使用 SMO 算法进行求解，与之前相比主要是加入了新的约束条件。</p><h4 id="3-2-超参数-C"><a href="#3-2-超参数-C" class="headerlink" title="3.2 超参数 C"></a>3.2 超参数 C</h4><p>&emsp;&emsp; 参数 C 用来在模型准确率与模型复杂度之间取得一个平衡。<strong>如果 C 比较大</strong>，要想达到最小化的目标，那么会使得松弛因子之和$\sum_{i=1}^N \xi_i$较小，此时模型会尽可能多地拟合训练样本，SVM 的决策间隔会比较小，同时对于极端值的敏感度增加，泛化能力较差；<strong>如果 C 比较小</strong>，那么就会鼓励模型去寻找一个较大间隔的决策边界，不强求对于训练数据完美得拟合，对极端值的容忍度比较高。此时会牺牲一定的准确性。</p><h3 id="4-非线性-SVM-——核函数"><a href="#4-非线性-SVM-——核函数" class="headerlink" title="4. 非线性 SVM ——核函数"></a>4. 非线性 SVM ——核函数</h3><h4 id="4-1-核函数"><a href="#4-1-核函数" class="headerlink" title="4.1 核函数"></a>4.1 核函数</h4><p>&emsp;&emsp; 虽然松弛变量能够允许一定的离群点，但是只适合近似线性可分的情况。对于数据线性不可分的情况，SVM 通过进行空间映射，将<strong>低维空间中的线性不可分问题转化为高维空间的线性不可分问题</strong>。</p><div align=center><img src="https://tva3.sinaimg.cn/large/006Ahf5Fly1gm751pis71j31cu0l9gnt.jpg"></div><div align=center>图 6. 空间映射示例</div><p>&emsp;&emsp; 将所有样本点从原来的不可分空间转换到新的可分特征空间，我们需要定义一个映射：$\Phi : x \rightarrow \Phi(x)$。核函数是一个函数，其定义如下：$$K(x_1, x_2) = &lt;\Phi(x_1), \Phi(x_2)&gt; \tag{23}$$</p><p>其中$&lt;\cdot , \cdot&gt;$表示两个向量的内积。</p><p>&emsp;&emsp; 核函数与在映射后的特征空间计算内积是等价的，这样的好处是可以直接在低维空间计算内积，不需要显示地进行空间映射，计算效率更高。常见的核函数有：</p><ul><li>线性核函数：$K(x_1, x_2) = &lt;x_1, x_2&gt;$</li><li>多项式核：$K(x_1, x_2) = (&lt;x_1, x_2&gt; + R)^d$</li><li>高斯核：$K(x_1, x_2) = exp(- \frac{\lVert x_1 - x_2 \rVert^2}{2 \sigma^2})$</li></ul><h4 id="4-2-非线性-SVM"><a href="#4-2-非线性-SVM" class="headerlink" title="4.2 非线性 SVM"></a>4.2 非线性 SVM</h4><p>非线性 SVM 对应的最优化问题为：$$\hat{\alpha} = argmax_{0 \leq \alpha \leq C} \ { \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i,j=1}^N y_i y_j \alpha_i \alpha_j K(x_i, x_j) } \tag{24}$$</p><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><ul><li>SVM 是一种最大化分类间隔的分类器</li><li>由于满足 KKT 条件，SVM 的优化问题通常可以转换为对偶问题求解</li><li>通过引入松弛变量，我们可以训练得到软间隔的 SVM</li><li>可以通过使用核函数让 SVM 处理线性不可分数据</li></ul><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li>刘洋老师——自然语言处理第5讲：支持向量机</li><li><a href="https://zhuanlan.zhihu.com/p/49331510" target="_blank" rel="noopener">看了这篇文章你还不懂SVM你就来打我</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> ML理论知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning基础 </tag>
            
            <tag> 支持向量机 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯决策理论</title>
      <link href="/2020/02/07/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA/"/>
      <url>/2020/02/07/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%90%86%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p>&emsp;&emsp; <strong>贝叶斯决策理论(Bayesian decision theory)</strong> 是统计方法处理模式分类问题的基本理论之一。在之前的学习过程中接触过该理论，但一直没理解其思想。本文试图探讨以下问题：</p><ul><li>贝叶斯决策理论是什么？有什么用处？</li><li>贝叶斯决策理论中”先验概率”，”后验概率”，”贝叶斯公式”是什么？</li><li>如果说贝叶斯决策理论是最优的，为什么还需要其他算法？</li></ul><hr><p>&emsp;&emsp;机器学习(Machine Learning)可以理解为<strong>通过观测数据$(X)$推测结果$(y)$，同时学习推断规则$(c)$，使得推断结果$\hat y = c(X)$与真实值$y$的误差尽可能小</strong>。举个例子，我们现在有一批西瓜(为简单起见，现假设仅通过根蒂就能判断西瓜的好坏)，并观察到了它们的根蒂形状$(X)$，就可以通过推断规则$(c)$来估计西瓜是好瓜还是坏瓜$(y)$。全文我们都会举西瓜的例子，我们能观察到的西瓜根蒂$X$(蜷缩、硬挺、稍蜷)，西瓜的好坏$y$：好瓜$(y=1)$、坏瓜$(y=0)$。</p><h3 id="1-在没有其他信息时，我们如何推断？"><a href="#1-在没有其他信息时，我们如何推断？" class="headerlink" title="1. 在没有其他信息时，我们如何推断？"></a>1. 在没有其他信息时，我们如何推断？</h3><ul><li>假设我们第一次接触判断西瓜好坏的问题，什么都不知道。此时只能预测好瓜和坏瓜的概率都是0.5。</li><li>假设我们虽然第一次接触判断西瓜好坏的问题，但神告诉我们这种西瓜是好瓜的概率$P(1) = 0.7$，是坏瓜的概率$P(0) = 0.3$。显然，此时我们倾向于预测西瓜是好瓜，且预测错误的概率是$1-P(1) = P(0) = 0.3$。</li></ul><p>&emsp;&emsp;此处的$P(1)=0.7$和$P(0)=0.3$就叫做<strong>先验概率(prior probability)</strong>，指的是在观测前我们就已知结果的概率分布$P(y)$。</p><h3 id="2-当有了更多信息时，该如何利用它们？"><a href="#2-当有了更多信息时，该如何利用它们？" class="headerlink" title="2. 当有了更多信息时，该如何利用它们？"></a>2. 当有了更多信息时，该如何利用它们？</h3><p>&emsp;&emsp;显然，前文的推测方式是很不准确的，因为没有考虑到西瓜的属性。而现实情况是我们往往可以观察到西瓜的一些属性，并非完全一无所知。因此，我们尝试回答：在观察到西瓜的根蒂形状后，它是好瓜的概率是多少？对应的数学表达式就是$P(y=1 \mid X)$。相应的，是坏瓜的数学表达式为$P(y=0 \mid X)$。</p><p>&emsp;&emsp;此处的$P(y=1 \mid X)$和$P(y=0 \mid X)$就叫做<strong>后验概率(posterior probability)</strong>，指的是在观测到$X$后我们对结果$y$的估计。可以简单地理解为，<strong>后验概率是对先验概率的一种”更加细致准确的刻画”，因为此时我们观测到了$X$，有了额外的信息，所以后验概率比先验概率更有意义</strong>。大部分机器学习模型尝试得到的就是后验概率。</p><h3 id="3-如何求得后验概率？"><a href="#3-如何求得后验概率？" class="headerlink" title="3. 如何求得后验概率？"></a>3. 如何求得后验概率？</h3><p>&emsp;&emsp;我们无法直接获得后验概率，但<strong>可以通过贝叶斯公式计算得到</strong>。根据条件概率易知：<br>$$P(A \mid B) = \frac{P(AB)}{P(B)} \Rightarrow P(AB) = P(A \mid B) \cdot P(B) \tag{1}$$ $$P(B \mid A) = \frac{P(BA)}{P(A)} \Rightarrow P(BA) = P(B \mid A) \cdot P(A) \tag{2}$$</p><p>因为$P(AB) = P(BA)$，所以$P(A \mid B) \cdot P(B) = P(B \mid A) \cdot P(A)$，进而推出了大名鼎鼎的<strong>贝叶斯公式</strong>：$$P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)} \tag{3}$$ 将其带入到上文中的后验概率中，可求得：$$P(y=1 \mid X) = \frac{P(X \mid y=1) \cdot P(y=1)}{P(X)} \tag{4}$$ $$P(y=0 \mid X) = \frac{P(X \mid y=0)\cdot P(y=0)}{P(X)} \tag{5}$$ 我们发现，在通过贝叶斯公式变形后，后验概率变得可以计算了。<strong>贝叶斯决策的预测值是选取后验概率最大的结果</strong>。</p><h3 id="4-为什么贝叶斯决策理论是”最优的”？"><a href="#4-为什么贝叶斯决策理论是”最优的”？" class="headerlink" title="4. 为什么贝叶斯决策理论是”最优的”？"></a>4. 为什么贝叶斯决策理论是”最优的”？</h3><p>&emsp;&emsp;因为$P(y=0 \mid X) = 1- P(y=1 \mid X)$，<strong>只要观察到$X$，并选择后验概率最大的结果，就可以最小化预测的错误概率。这个结论对所有的观测值$X$均成立，所以才叫做”最优”</strong>。</p><h3 id="5-贝叶斯决策理论有什么用？"><a href="#5-贝叶斯决策理论有什么用？" class="headerlink" title="5. 贝叶斯决策理论有什么用？"></a>5. 贝叶斯决策理论有什么用？</h3><p>&emsp;&emsp;我们定义最小误分概率(最优情况下):$$R^* = \sum_X P(X) \cdot min(P(y=0 \mid X), P(y=1 \mid X)) \tag{6}$$ 易知$\sum_X P(X)=1$且$0 \leq P(X_i) \leq 1$。在二分类的情况下$0 \leq min(P(y=0 \mid X), P(y=1 \mid X)) \leq 0.5$，因此最小误分概率$0 \leq R^* \leq 0.5$。<br>&emsp;&emsp;举个例子，假设在所有的观测$X$下，$min(P(y=0 \mid X), P(y=1 \mid X)) \leq 0.2$成立，那么我们可以知道这个决策的最大误分概率就是$0.2$，即<strong>不存在任何其他决策规则会得到低于$0.2$的误分概率</strong>。所以贝叶斯决策理论可以用来<strong>估算理论上的计算边界(Bound)，尤其是误差的上界</strong>。</p><h3 id="6-如果说贝叶斯决策理论是最优的，为什么还需要其他算法？"><a href="#6-如果说贝叶斯决策理论是最优的，为什么还需要其他算法？" class="headerlink" title="6. 如果说贝叶斯决策理论是最优的，为什么还需要其他算法？"></a>6. 如果说贝叶斯决策理论是最优的，为什么还需要其他算法？</h3><p>&emsp;&emsp;既然贝叶斯决策理论是最优的，为什么不能盖过深度学习的锋芒呢？其根本原因在于贝叶斯决策中假设比较强，实际操作起来并不容易：</p><ul><li>无法获得真实的先验概率等其他计算要素，需要通过统计学的方法进行估计，存在一定的误差。</li><li>计算所需要的概率往往是不可能的，会面临维度灾难(the curse of dimensionality)。在条件概率$P(y=0 \mid X)$和$P(y=1 \mid X)$中，要考虑观察的特征不止一个，可能还有其他特征。而且实际计算时往往面临很多数值稳定性问题，所以朴素贝叶斯才会介绍平滑(smoothing)来解决一些特例。</li></ul><p>&emsp;&emsp;综上所述，<strong>直接利用贝叶斯决策理论进行后仰概率估算是不大实用的，但对于理论上计算边界，尤其是误差的上界比较有意义</strong>。</p><hr><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><ul><li><a href="https://www.zhihu.com/question/27670909/answer/378129817" target="_blank" rel="noopener">如何简单理解贝叶斯决策理论?</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> ML理论知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning基础 </tag>
            
            <tag> 理论知识 </tag>
            
            <tag> 贝叶斯决策理论 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title>关于</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="关于博客"><a href="#关于博客" class="headerlink" title="关于博客"></a>关于博客</h2><hr><ul><li>主要记录自己日常的学习笔记、论文笔记等内容</li><li>目前博客很简陋，后期看情况折腾吧</li><li>此博客由<a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>强力驱动，并部署在<a href="https://github.com/" target="_blank" rel="noopener">Gayhub上</a>(逃)</li></ul><h2 id="博客日志"><a href="#博客日志" class="headerlink" title="博客日志"></a>博客日志</h2><hr><ul><li>2020-01-18 建站</li><li>2020-01-19 主题优化，添加搜索、评论等功能</li><li>2020-01-20 添加相册功能</li></ul><h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><hr><ul><li>95后，目前大四，已确定读研</li><li>本科就读于东北大学(沈阳)计算机科学与技术专业</li><li>硕士就读于中科院计算所(网络数据科学与技术实验室)，方向NLP</li><li>邮箱：<a href="mailto:zhangyqCS@163.com">zhangyqCS@163.com</a></li></ul>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>相册</title>
      <link href="/photos/index.html"/>
      <url>/photos/index.html</url>
      
        <content type="html"><![CDATA[<h2 id="❤NEU"><a href="#❤NEU" class="headerlink" title="❤NEU"></a>❤NEU</h2><!-- <div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column"  style="width: 33.333333333333336%;"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfhre7wj31z418g4qs.jpg" alt="樱"></div><div class="group-picture-column"  style="width: 33.333333333333336%;"><img src="http://tva2.sinaimg.cn/large/006Ahf5Fly1gb1yfiviekj31hc0xcn82.jpg" alt="晚霞"></div><div class="group-picture-column"  style="width: 33.333333333333336%;"><img src="http://tva4.sinaimg.cn/large/006Ahf5Fly1gb1yfjd0efj31qu0zkdoz.jpg" alt="傍晚"></div></div><div class="group-picture-row"><div class="group-picture-column"  style="width: 100%;"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfm15e1j31z418g7wi.jpg" alt="海贼王"></div></div></div></div><p> –&gt;</p><!-- <center>    <figure>        <img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfhre7wj31z418g4qs.jpg" width="300px" height="200px" />图1        <img src="http://tva2.sinaimg.cn/large/006Ahf5Fly1gb1yfiviekj31hc0xcn82.jpg" width="300px" height="300px" />图2    </figure></center> --><table>    <tr>        <td><img src="http://tva3.sinaimg.cn/large/006Ahf5Fly1gb348qljo4j31900s0aff.jpg" ></td>        <td><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb34cl6363j33401yghdv.jpg" ></td>        <td><img src="http://tva1.sinaimg.cn/large/006Ahf5Fly1gb348qpfwgj31990s0wls.jpg" ></td>    </tr><!--     <tr>        <td colspan="2"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb34cl6363j33401yghdv.jpg" ></td>    </tr>    <tr>        <td colspan="2"><img src="http://tvax3.sinaimg.cn/large/006Ahf5Fly1gb1yfm15e1j31z418g7wi.jpg" ></td>    </tr> --></table>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>标签</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
